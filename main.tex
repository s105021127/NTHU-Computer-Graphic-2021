\documentclass[12pt,a4paper]{report} 
\usepackage[utf8]{inputenc}
\usepackage{background}
\usepackage{style/nthu_format} 
\usepackage{xcolor}
\usepackage{natbib}
\usepackage{url}
\usepackage{enumerate}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage[section]{placeins}
\usepackage{times}
\usepackage{graphicx}
\usepackage{algorithmic}

\usepackage{amsmath, amsthm, amssymb}
%\usepackage{subfigure}
\usepackage{url}
\usepackage[encapsulated]{CJK} 
\usepackage[nottoc,notlot,notlof]{tocbibind}
\usepackage{tocloft}% Contents vspace
\renewcommand{\cftchapleader}{\cftdotfill{\cftdotsep}}
%\usepackage{titlesec}% http://ctan.org/pkg/titlesec 3/19
%\usepackage{lipsum}% http://ctan.org/pkg/lipsum 3/19
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]


\backgroundsetup{
scale=1.5
,
contents= {\includegraphics{images/nthu_logo}}
}
%-------------------------------------------------------------------------
% take the % away on next line to produce the final camera-ready version
\pagestyle{empty}

%-------------------------------------------------------------------------
\begin{document}
\begin{CJK}{UTF8}{bsmi}



\beforepreface 

%-------------------------------------------------------------------------
\clearpage\maketitle
\thispagestyle{empty}
%-------------------------------------------------------------------------

\afterpreface

%\titleformat{\chapter}{\Huge\bfseries}{\chaptername\ \thechapter}{0pt}{\vskip 20pt\raggedright}% 3/19 add vspace
%\titlespacing{\chapter}{0pt}{50pt}{\parskip}% 3/19 add vspace



\chapter{Introduction}
    
    \par Sarcasm has been challenging the sentiment analysis community for a while now~\cite{feldman2013techniques,maynard2014cares,farias2016irony}.
    The existence and permanent proliferation of so-called internet trolls together with multiple platforms for them to operate in has not made this challenge easier.
    It thus becomes useful to devise systems that can automatically detect sarcastic intentions in texts.
    % \par Sarcasm has been challenging the sentiment analysis community for a while now. The existence and permanent proliferation of so-called internet trolls together with multiple platforms for them to operate in has not made this challenge easier. Social media platforms have for long been regarded as a rich data source, especially since it is possible to understand opinions and emotions expressed in them towards a particular subject or object. Nevertheless, these expressions can be potentially unreliable if the presence of sarcasm can make them mean the exact opposite. It thus becomes useful to devise systems that can automatically detect sarcastic intentions in texts.
    %\par Because of the trend of social network, people are used to post their instant status on social media. The big amount of status messages benefit the research of semantic analysis.\cite{agarwal2011sentiment,shaikh2017tweet} In the semantic analysis field, for a company, they get the feedback of their products through analyzing the social media messages which talking about their product , find the opinion and emotion from their target user. Pattern extraction could help to retrieve the core information in a message.\cite{kawabata2016frequent,lee2016evaluative} 
    
    \par When trying to detect sarcasm, context plays an important role, which implies an understanding of several factors in the setting of a comment~\cite{bamman2015contextualized,joshi2015harnessing}.
    The topic, background of the user, background of the receiver, and emotions conveyed can provide some insight when determining if a comment is sarcastic or not. 
    % as can be observed in Figure~\ref{fig:example}.
    The main problem is that not all of these factors are available when attempting to train an algorithm to detect sarcasm.
    Moreover, even if these contextual cues are available to a human reader, sarcasm may still be hard to detect.
    It is thus helpful to look into other features that can provide clues or hints on the presence of sarcasm.
    
    % \par When trying to detect sarcasm, context plays an important
    % role, which implies an understanding of several factors in
    % the setting of a comment. The topic, background of the
    % user, background of the receiver, and emotions conveyed
    % can provide some insight when determining if a comment is
    % sarcastic or not. The main problem is that not all of these
    % factors are available when attempting to train an algorithm
    % to detect sarcasm. Moreover, even if these contextual cues
    % are available to a human reader, sarcasm may still be hard
    % to detect. It is thus helpful to look into other features that
    % can provide clues or hints on the presence of sarcasm.
    %\par There are some research\cite{chatterjee2017rent,kouloumpis2011twitter} do the stop words removal in the preprocessing step for pattern extraction.This observation has brought us to the hypothesis that all the stop words have no relevant to emotion. However, even though the stop words are not explicit relevant to emotion, they do have the emotion as they combining together and shown in the messages. e.g. "* is for" is a pattern formed by a Be verb and a Prepositions, matching the messages like "Everything I do is for you.". That's a hope emotion. 
    
    \par Some of these features may be related to the social media platforms themselves.
    For example, what is the role of anonymity when incurring in sarcastic commenting? This may have an impact.
    A user might feel more comfortable being sarcastic on a public page full of strangers than on a private chat with close friends.
    Other users may also prefer it the other way around.
    What seems to be certain, however, is that sarcasm is more prevalent as a retaliatory move than as an initial exchange.
    It is more likely to find a sarcastic reply or comment than a sarcastic post, that is, as a response to try to outsmart an original post~\cite{mueller2016positive}.
    \par Social media platforms have for long been regarded as a rich data source, especially since it is possible to understand opinions and emotions expressed in them towards a particular subject or object.
    Facebook has been for some time now one of the leading online social networks~\cite{wilson2012review}.
    %Its permanent development makes them add     different options continuously.
    One of the platform's features, Facebook Pages, provides an adequate setting for subjective comment behavior mentioned above.
    Pages are in essence official accounts for an individual, media, or organization.
    Posts made in these pages receive more views and comments than regular user accounts, and now, due to one of the latest additions, more reactions.
    Reactions allow users not only to comment but also to express one of the emotions elicited in Figure~\ref{fig:reactions}, in addition to the Like button.
    This has made Facebook Pages posts a place laced with opinions, emotions, and sarcasm.
    It is important to understand how these interact together.
    It can be said that sarcasm is used to invert emotions, but conversely, can inverted emotions be an indicator of sarcasm?
    % \par Some of these features may be related to the social media
    % platforms themselves. For example, what is the role of
    % anonymity when incurring in sarcastic commenting? This
    % may have an impact. A user might feel more comfortable
    % being sarcastic on a public page full of strangers than on a
    % private chat with close friends. Other users may also prefer
    % it the other way around. What seems to be certain, however,
    % is that sarcasm is more prevalent as a retaliatory move than
    % as an initial exchange. It is more likely to find a sarcastic
    % reply or comment than a sarcastic post, that is, as a response
    % to try to outsmart an original post.
    %\par Those emotion patterns constructed by stop words help emotion detection contributing to emotion classification work.

    % \par Facebook has been for some time now the leading online social network. One of the platform’s features, Facebook Pages, provides an adequate setting for the sarcastic comment behavior mentioned above. Pages are in essence official accounts for an individual, media, or organization. Posts made in these pages receive more views and comments than regular
    % user accounts, and now, due to one of the latest additions,
    % more reactions. Reactions allow users not only to comment
    % but also to express one of the emotions elicited in Figure 1, in addition to the Like button. This has made Facebook Pages
    % posts a place laced with opinions, emotions, and sarcasm.
    % It is important to understand how these interact together.
    % It can be said that sarcasm is used to invert emotions, but
    % conversely, can inverted emotions be an indicator of sarcasm?
    % \par In this research, we discover the Implicit Emotion Patterns which are the emotion patterns without any emotion-related words defined by dictionaries. We assume those patterns have high relevant as the threshold set to the emotion score of specific emotion, and evaluating by 6 annotators agreeing on the emotions of the messages are the same as the emotions of the patterns inside the messages. For example, one of the Implicit Emotion Patterns "why does my" matching the messages like "Why does my iphone always crash when I need it?" or "Why does my sister just can't help at all?" is highly relevant to "Anger".
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{images/facebook-reactions.png}
        \caption{Facebook reactions used as noisy labels on comments for emotion detection.}
        \label{fig:reactions}
    \end{figure}

    \par This work tries to make use of the inherent characteristics of the Facebook platform with the objective of developing a system for emotion detection based only on the content extracted without the need of external knowledge.
    In essence, it leverages the wisdom of the crowds in order to achieve its goal.
    First, it uses the intersection of reaction clicks and comments as a fuzzy labeling technique with distant supervision where the reactions become the labels of the comments to train our emotion classifier.
    It is then explored if the presence of opposing emotions with regards to a comment is an indicator of sarcasm.
    Experiments have been performed for both English and Chinese comments and an extended evaluation for English is presented.
    To the best of our knowledge, the self-reported reactions have not been previously used as emotion signals for labeling, nor has the possibility of detecting sarcasm from this kind of classifier been explored before.
    % \par This work tries to make use of the inherent characteristics of the Facebook platform with the objective of defining a method for automatic sarcasm detection. In essence, it leverages the wisdom of the crowds in order to achieve its goal. First, it uses the intersection of reaction clicks and comments as a fuzzy labeling technique with distance supervision where the reactions become the labels of the comments. An emotion classifier is consequently built on this data. It is then explored if the presence of opposing emotions with regards to a comment is an indicator of sarcasm. Experiments have been performed for both English and Chinese comments and an extended evaluation for English is presented. To the best of our knowledge, the self-reported reactions have not been previously used as emotion signals for labeling, nor has the possibility of detecting sarcasm from this kind of classifier been explored before.

%\par The rest part of this paper, we will describe our Related Work in Chapter 2, Methodology in Chapter 3, Experiment & Result in Chapter 4 and also the conclusion in Chapter 5.

\chapter{Related Work}
    \section{Emotion Analysis on Social Media}
    %\textbf{Pattern Extraction}\newline
   
    \par Online social media platforms have increasingly attracted more interest for the sentiment and emotions expressed in their users' opinions. This has led to the inclusion of several explicit means to reflect such emotion in a comprehensive, user-friendly and collectible way. Several works have focused on using these signals as noisy labels for emotion classification~\cite{argueta2016multilingual, davidov2010enhanced, wang2012emotion,tang2013emotion, go2009twitter, hu2013unsupervised, wang2016sentiment, zhao2012moodlens}. The work by Go et al.~\cite{go2009twitter}  evaluated the performance of popular machine learning algorithms when using emoticons in tweets as labels for training via distant supervision. Davidov et al.~\cite{davidov2010enhanced}  leveraged Twitter features for sentiment learning and not only considered emoticons as labels, but also added hashtags. In a similar way, Wang et al.~\cite{wang2012emotion} automatically collect a large emotion labeled data set by using 7 category emotion hashtags which are available in the tweets, and evaluated the performance of automatically emotion identification.
    
    %\par Online social media platforms have increasingly attracted more interest for the sentiment and emotions expressed in their users’ opinions. This has led to the inclusion of several explicit means to reflect such sentiment and emotion in a comprehensive, user-friendly and collectible way. Several works have focused on using these signals as noisy labels for emotion classification [1, 4, 6, 7, 22, 23]. The work by Go et al. [6] evaluated the performance of popular machine learning algorithms when using emoticons in tweets as labels for training via distant supervision. In a similar way, Davidov et al. [4] leveraged Twitter features for sentiment learning and not only considered emoticons as labels, but also added hashtags.
    %\par  Narr et. al. utilize n-gram method, which tokenized the messages into n words making the computation time much longer .\cite{pak2010twitter,davidov2010enhanced} 
    %\par The previous works confirmed at the time that social media could provide not only data, but annotated data that could avoid the time- and resource-intensive task of manual annotation. This advantage was further explored by Zhao et al. [23] using data from a different platform (Weibo) and language (Chinese). Their system, MoodLens, maps 95 emoticons into 4 sentiment classes and became one of the pioneering tools for sentiment analysis from short texts in Chinese. Another study using the Weibo platform performed sentiment correlation to determine which of two emotions—anger and joy–is    more influential in a social network [5]. Lipsman et al. [10] focused uniquely on the number of Likes in a post to determine what kind of repercussions this click behavior had from the perspective of brands and fans.
    \par The previous works confirmed at the time that social media could provide not only data, but annotated data that could avoid the time- and resource-intensive task of manual annotation. This advantage was further explored by Tang et al.~\cite{tang2013emotion} using data from a different platform (Weibo) and language (Chinese). They filtered out the ambiguous emoticons, and separate the remaining emoticons into 4 emotion classes and became one of the pioneering tools for emotion analysis from short texts in Chinese.Another study using the Weibo platform performed sentiment correlation to determine which of two emotions—anger and joy–is more influential in a social network ~\cite{fan2014anger}. Lipsman et al.~\cite{lipsman2012power}  focused uniquely on the number of Likes in a post to determine what kind of repercussions this click behavior had from the perspective of brands and fans.
    %\par  Argueta et. al.\cite{argueta2016multilingual} used unsupervised graph-based method to extract the patterns been used frequently in the daily messages. There are two kind of words in a pattern. One is connect words(cw) and the other is psychology words(pw). And they are build by graph-based method. In that method words are been classified by the node centrality and clustering coefficient. The nodes with high node centrality tend to be connected word which included stop words. And the nodes with high clustering coefficient tend to be psychology words. By combining at least one cw and one pw word, we can generate the patterns containing stop words.
        
        
        %\par Following a similar trend, the work by Hu et al. [7] studied the use of emotion signals not only as labels for training, but also as an active part in unsupervised learning models for sentiment analysis. Hashtags on its own have also been used for similar tasks. Argueta et al. [1] used hashtags for distant supervision on unsupervised methods to collect writing patterns that can be correlated to emotions. It has been found that using emoticons or hashtags as labels can lead to some errors. This served as motivation for Wang et al. [22], who proposed a method for “de-noising” the obtained labels.
        
        \par Following a similar trend, the work by Hu et al.~\cite{hu2013unsupervised}  studied the use of emotion signals not only as labels for training, but also as an active part in unsupervised learning models for sentiment analysis. Hashtags on its own have also been used for similar tasks. Argueta et al.~\cite{argueta2016multilingual} used hashtags for distant supervision on unsupervised methods to collect writing patterns that can be correlated to emotions. It has been found that using emoticons or hashtags as labels can lead to some errors. This served as motivation for Wang et al. ~\cite{wang2016sentiment}, who proposed a method for “de-noising” the obtained labels.

        %\par This method save much computation time and also have the smaller size of training data set. So we utilize their method to get our original emotion pattern list from social media, Twitter.And we consider these 6 emotions as our target: Joy, Sadness, Anger, Fear, Hope and Surprise.\cite{plutchik2001nature} By generating the pattern list which containing two parts: the patterns extracting from the data set (social media message) and the relative emotion scores which is the extension of tf-idf representing how strong the intensity of emotions. e.g. The pattern "* I hope" with the highest scores in Hope and Fear. While looking the patterns in detail , some of them are highly relevant to emotions which are easy for human to feel the emotions in the message. For example, the pattern containing "holiday" is related to the emotion "Joy". And the pattern containing "exam" is related to "Fear".\cite{hunte2013term} They set an emotion score for each pattern for each emotion. The higher the score is the stronger the emotion intensity is. These patterns with emotion scores composed a pattern list. We utilize this list for  further discover the Implicit Emotion Patterns.\newpage
        %\par Despite the availability of multiple online social networks, most of the related work has been focused on Twitter. Ortigosa et al. [15] were among the first to perform sentiment analysis on Facebook. Their application—SentBuk—tries to help e-learning systems by providing sentiment information of users through their posts. The achieved performance shows that Facebook data can also be used for sentiment related tasks.
        \par Despite the availability of multiple online social networks, most of the related work has been focused on Twitter. Rangel et al.~\cite{rangel2013emotion}  do the Identification of Emotions and Authors’ Gender in Facebook Comments. They presents a method for automatically identifying emotions in Facebook and in Spanish language, taking into account another dimension of personality - the gender of the author.
        Ortigosa et al.~\cite{ortigosa2014sentiment}  perform sentiment analysis on Facebook. Their application—SentBuk—tries to help e-learning systems by providing sentiment information of users through their posts. The achieved performance shows that Facebook data can also be used for sentiment and emotion related tasks.
        
        
        %\par Recent years have witnessed the development of algorithms that deliver very high performance on sentiment related tasks. VADER, the rule-based model developed by Hutto and Gilbert [8], tries to make the most out of sentiment lexicons combined with machine learning algorithms. Deep convolutional neural networks have also had a significant participation in sentiment classification tasks. The work by [17] presents a model for multi-modal classification of short sentences based on features extracted from text. As highlighted by Liu [11] and Cambria [3], however, there are several factors weighing in on sentiment related topics, many of which have not been thoroughly explored. For instance, Volkova et al. [21] tried to explore the impact of demographic language variations when attempting multilingual sentiment analysis, and made clear how this can be an issue. The context on which opinions are expressed is also of high importance, as studied by Muhammad et al. [14]. Their work explains that depending on the social media genre being studied, significant variations in the modeling are required. The impact of innate human responses such as sarcasm is also one of the factors pending an in-depth exploration.
        
    \section{Sarcasm Detection}
    %\textbf{Word Dictionary} \newline
    \par Sarcastic expressions are a natural product of humor improvisation and have found a natural proliferation space in online social media. With them, they bear a lot of trouble for mining tasks due to the uncertainty and ambiguity they bring to expressions. If it is hard for humans to define and identify sarcasm, it is even harder to teach a computer how to do it. Nevertheless, the research community has done efforts to achieve this.
    %\par Some research will use the LIWC (Linguistic Inquiry and Word Count) to further analyze the emotion inside the messages. LIWC is a emotion related dictionary built on previous research establishing strong links between linguistic patterns and personality or psychological state, but makes possible far more detailed results than did manual counts. Some research\cite{li2012comparative} adapted LIWC dictionary for judging whether a word is related to emotion or not. The ways that individuals talk and write provide windows into their emotional and cognitive worlds. Over the last four decades, researchers have provided evidence to suggest that people's physical and mental health are correlated with the words they use \cite{gottschalk1969measurement,mackenzie1978manipulative}. More recently, a large number of studies have found that having individuals write or talk about deeply emotional experiences is associated with improvements in mental and physical health \cite{lepore2002writing} Text analyses based on these studies indicate that those individuals who benefit the most from writing tend to use relatively high rates of positive emotion words, a moderate number of negative emotion words, and an increasing number of cognitive words, and switch their use of pronouns from writing session to writing session \cite{campbell2003secret,pennebaker1997linguistic}.
%In order to provide an efficient and effective method for studying the various emotional, cognitive, and structural components present in individuals' verbal and written speech samples, they originally developed a text analysis application called Linguistic Inquiry and Word Count, or LIWC.  The first LIWC application was developed as part of an exploratory study of language and disclosure \cite{guillemin1993cross,pennebaker1993putting}. The second version, LIWC2001, updated the original application with an expanded dictionary and a more modern software design \cite{pennebaker2001linguistic}. The most recent evolution, LIWC2007, has significantly altered both the dictionary and the software options. As with previous versions, however, the program is designed to analyze individual or multiple language files quickly and efficiently.  At the same time, the program attempts to be transparent and flexible in its operation, allowing the user to explore word use in multiple ways.
\par Maynard and Greenwood~\cite{maynard2014cares}  highlight the importance of
understanding the impact of sarcasm in sentiment analysis.
Reyes et al.~\cite{reyes2012humor}  first attempted to identify humor and irony,
as this could provide some insights to sarcastic expressions.
Based on textual features and leveraging on the hashtags \#humor and \#irony, they developed a system to identify
“figurative language”. Bamman and Smith~\cite{bamman2015contextualized}  believe that
sarcasm is a highly contextual phenomenon and that extralinguistic
information is required for its detection. They
consider lexical cues and their corresponding sentiment as
contextual features in their study. Rajadesingan et al.~\cite{rajadesingan2015sarcasm} 
go beyond these affirmations and claim behavioral traits are
also intrinsic to users expressing sarcasm. They developed
a model for sarcasm detection based on the analysis of past
tweets paired with behavioral and psychological studies.
Riloff et al.~\cite{riloff2013sarcasm} attempts to identify situations in which a situation and its subsequent reaction have opposing sentiment polarities. They use this as a clue to identify sarcastic expressions using bootstrap learning methods. Gonzalez-Ibanez et al.~\cite{gonzalez2011identifying} also experimented with the sentiment polarity in twitter messages and the presence of sarcasm transforming this polarity. Their work uses lexical and pragmatic features to train a machine learning system to identify these utterances. Lexical feature where also used by Bharti et al.~\cite{bharti2015parsing} in developing their  parsing-based lexicon generation algorithm to detect sarcasm on twitter.
%\par The LIWC2007 Dictionary is the heart of the text analysis strategy.  The default LIWC2007 Dictionary is composed of almost 4,500 words and word stems.  Each word or word stem defines one or more word categories or sub dictionaries.  For example, the word cried is part of five word categories:  sadness, negative emotion, overall affect, verb, and past tense verb.  Hence, if it is found in the target text, each of these five sub dictionary scale scores will be incremented.  As in this example, many of the LIWC2007 categories are arranged hierarchically.  All anger words, by definition, will be categorized as negative emotion and overall emotion words.  Note too that word stems can be captured by the LIWC2007 system.  For example, the LIWC2007 Dictionary includes the stem hungr* which allows for any target word that matches the first five letters to be counted as an ingestion word (including hungry, hungrier, hungriest).  The asterisk, then, denotes the acceptance of all letters, hyphens, or numbers following its appearance.
%Each of the default LIWC2007 categories is composed of a list of dictionary words that define that scale. We utilize LIWC2007 to ignore the patterns containing explicit emotion words.  
\par Sarcasm detection has also been attempted in other languages.
The work by Lunando and Purwarianti~\cite{lunando2013indonesian} first
performs sentiment classification on short texts from Indonesian
social media. It then considers two other features:
negativity information and the number of interjection words to perform sarcasm detection through machine learning algorithms.
Liebrecht et al.~\cite{liebrecht2013perfect}  built a Twitter-based corpus
by collecting tweets containing the hashtag \#sarcasm and
trained a machine learning classifier. The data used was
in Dutch, but still showed that sarcasm is often signaled
by intensifiers and exclamation marks. Tweets in English
and Czech were studied by Ptacek et al.~\cite{ptacek2014sarcasm} to develop a
language-independent approach borrowing features across
languages.
The work by Liu~\cite{liu2014sarcasm} explores sarcasm detection on Chinese text primarily focusing on the issue of imbalanced data and proper feature selection which is evaluated through multiple classifiers. The dataset used contain simplified Chinese characters, to the best of our knowledge our work is the first to address this problem for traditional Chinese characters.
%\newpage  \par On the other hand, we also consider the synonyms of the emotion words collected from LIWC. There is a dictionary called WordNet. WordNet is a lexical database for the English language. It groups English words into sets of synonyms called synsets, provides short definitions and usage examples, and records a number of relations among these synonym sets or their members. WordNet can thus be seen as a combination of dictionary and thesaurus. As of November 2012 WordNet's latest Online-version is 3.1. The database contains 155,287 words organized in 117,659 synsets for a total of 206,941 word-sense pairs; in compressed form, it is about 12 megabytes in size. WordNet includes the lexical categories nouns, verbs, adjectives and adverbs but ignores prepositions, determiners and other function words. Words from the same lexical category that are roughly synonymous are grouped into synsets. Synsets include simplex words as well as collocations like "eat out" and "car pool." The different senses of a polysemous word form are assigned to different synsets. The meaning of a synset is further clarified with a short defining gloss and one or more usage examples. An example adjective synset is: good, right, ripe – (most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes"). All synsets are connected to other synsets by means of semantic relations. 
    %-other Implicit emotion work

    
 \chapter{Methodology}
    \section{Overview}
    Taking from the approach mentioned in the related work, the emotion classifier is an adaptation of the work by Argueta et al.~\cite{argueta2016multilingual}. The training data and labels are obtained from the intersection between reaction clicks and comments from users corresponding to those reactions. The classifier returns the two most likely labels corresponding to a text. These two labels are then analyzed to determine if it is a sarcasm candidate. The final evaluation will classify the comment as being sarcastic or not. A flowchart for the method is presented in Figure~\ref{fig:flowchart}.
    %To examine whether the emotion patterns constructed by stop words having relevant to emotions or not. Extract the emotion patterns from the social media and filter out the patterns having emotion words. With those patterns, set the thresholds of emotion score for each emotion to get the patterns unique in specific emotion. Then randomly collect the tweets data which containing those emotion specific patterns to annotate the emotions.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.5]{images/overview.pdf}
        \caption{Methodology flowchart.}
        \label{fig:flowchart}
    \end{figure}
    \section{Data Collection}
    \par One of the key features of this work is the exploitation of embedded characteristics of the Facebook platform, the first being their “Pages” feature. Facebook Pages are official accounts of a varied types of sources, popular personalities, organizations, and media outlets. Our methodology takes particular advantage of the official pages of news media. The implemented emotion classification algorithm requires objective and subjective data in its development. By using pages from news media, objective texts can be obtained from news posts, while subjective texts can be obtained from user comments. It is intuitive that comments on these articles are usually highly opinionated, sometimes biased, and predominantly subjective.
    
    \par The second key element to be used is Facebook Reactions. Since the beginning of 2016, the traditional “Like” button was replaced by a more variety-aware option called Reactions. Reactions are emoji-based expressions that allow a user to express their sentiment towards a post. It was identified that many of the users who react to posts also have a tendency to comment on them. The proposed data collection approach is to find the intersection between reaction clicks and comments that will enable a matching between a user comment and its corresponding reaction. This not only allows a filtering process to build a collection, but also guarantees that there will be a self-reported emotion assigned to every comment, which basically results in an automatic annotation.
    
    \par The previously mentioned characteristics enable the collection of objective news data and subjective comments data, the latter which is paired with emotion labels. This meets the requirements for the implementation of the pattern-based emotion classifier to be used in this work.
%Definition 1:
%    An element \(e\) is a word or a sequence of symbols (,.?!, etc).\newline
%    Definition 2:
%    A pattern \(P_i\) is a sequence of three elements.\newpage
%    \begin{center}
%        {T}  represents any message in social media.\newline
%    \(ws\) is a word sequence in text.
%    \[{ws}=\{{w_1,w_2,w_3}\}\in {T}, \forall{w}_i \in {ws} \text{,is a word}.\]
%    \[\mathcal{EW} \text{ is a subset of emotion words defined in LIWC.}\]
%    \[{ws_x}\text{ is a word sequence without emotion words.}\]
%    \[{ws_x} \cap \{{EW}\} = \emptyset\]    
%    \[\{{EMO}\}\text{ is a set of emotions.}    \]
%    \[\{{EMO}\}=\{{emo_1,emo_2,...,emo_n}\}=\{{joy,anger,surprise,sadness,fear,hope}\}\]
%    \[{getEmo()} \text{ is an emotion classification function.}\]
%    \[getEmo({ws_x})=emo_i\]
%    \end{center}
    
    
%     \[{P_i} = [e_1, e_2, e_3] \]
%   \[
%    \forall {P_i} \in \mathcal{P}\]
%    Definition 3:
%    A pattern list \(P\) is a list containing all the emotion patterns.
%   \[
%    \quad \mathcal{P} = \{P_1, P_2, P_3, ... ,P_n\}
%    \]
%    \newline
%    Definition 4: An Emotion Scores \(ES(emo,p)\) is a score representing how a pattern is related to an specific emotion.
%     \[   
%    ES(emo,p) \longmapsto es, es \in \mathbb{R^+}
%    \]
%    Definition 5:
%    An emotion\(emo\) is defined in a set of 6 emotions.\newline
%   All the patterns have 6 Emotion Scores(ES) for 6 Emotions.
%    \[
%    emo \in Emotion \{Joy, Anger, Sadness, Hope, Surprise, Fear\}
%    \]
    
    \section{Reaction-Based Emotion Classification}
     %\section{Framework}
    \par As previously mentioned, the emotion classifier used is an adaptation of previous work [1]. In general, the system builds a graph with subjective terms from short text. It then extracts patterns of expression from this graph and assigns weights to them across a multiple range of emotions. Being pattern based, it allows for multi-lingual analysis. The implementation used in this work varies from the original in the following aspects:
    \begin{itemize}
        \item Facebook data is used, rather than Twitter data.
        \begin{itemize}
            \item Focus on Facebook news comments, not personal statement as previous 	work.
        \end{itemize}
        \item Emotions from Facebook Reactions are used as labels instead of Plutchnik’s emotion set.
        \begin{itemize}
            \item Not every language use hashtags to indicate their emotion.
            \item Reaction-Comment pairs are considered as annotations rather than Hashtag-	Tweet pairs.
        \end{itemize}
        \item Chinese language is integrated.
    \end{itemize}
    \subsection{Graph Construction.}
    The data obtained, as defined in the previous section, is converted into graph form for more convenient manipulation. The nodes in the graph correspond to words, and the edges denote the co-occurrence between the connected words. The graph obtained from the news posts intuitively contains more factual, objective expressions, while the graph from the comments is more subjective and opinionated. Figure~\ref{fig:subj_graphs} shows examples of graphs constructed from comments (i.e., subjective) in Chinese and English.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{images/chinese_graph.jpg}
        \includegraphics[scale=0.7]{images/english_graph.jpg}
        \caption{Examples of subjective graphs from Chinese and English comments.}
        \label{fig:subj_graphs}
    \end{figure}
    \begin{CJK}{UTF8}{bsmi}
    In Chinese, there is a character reputation which is different from English. For example, the words in one to three characters are 喔、喔喔、喔喔 separately. Since they are the combination of the same character, but the meaning of them are totally different. For the word which is combined by more than three same word, the meaning is considered the same as the three character one. An example is given below.
    \newline
    \begin{CJK}{UTF8}{bsmi}
    \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|}
        \hline
            Sentence & Meaning \\
            \hline
            喔，我來做。& Negative Response \\
            \hline
            喔喔，我來做。& Neutral Acception \\
            \hline
            喔喔喔，我來做。& Positive Notification \\
            \hline
            喔喔喔喔，我來做。& Positive Notification \\
            \hline
    \end{tabular}
    \caption{Example of Chinese stacked word. Since the word sequence which contain three and four character 喔 has same meaning. Then four word sequence need to reduce to three word. }
    \end{table}
    \end{CJK}
    According to the characteristic mentioned above, we propose a word reduction on our Chinese data set while constructing the graph.
    \end{CJK}
    \begin{definition}{Chinese Stacked word Reduction}
    \newline
    For character sequence \(CS\) which is composed by more then three same character \(c\) in a row, then we reduce it into three character \(c\).
    \end{definition}
    \par The next goal is to obtain a set of expressions that are highly subjective so that they can be linked to particular emotions. With this purpose, a reduction is performed on the comments graph by removing terms that are highly dominant in the news graph. This procedure reduces the objective components present in the comments graph, resulting in a highly subjective graph. An example of graph reduction is shown in Figure~\ref{fig:chinese_graph_reduction} and Figure~\ref{fig:english_graph_reduction} for both Chinese and English graphs.
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{images/chinese_reduce.jpg}
        \caption{Chinese graph reduce.}
        \label{fig:chinese_graph_reduction}
    \end{figure}
    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{images/english_reduce.jpg}
        \caption{English graph reduce.}
        \label{fig:english_graph_reduction}
    \end{figure}
    \par With these graphs, frequent words can be extracted base on adjacency matrix, eigenvector centrality and clustering coefficient proposed by Argueta et. al.[1].
    % Eigenvector Centrality is a measurement to show the influence and importance of a node.
    % Clustering Coefficient is a measurement to show the level of how a node tend to group together with other nodes.
    In order to use these two measurement, the adjacency matrix is needed to be defined.
    \theoremstyle{definition}
    \begin{definition}{Adjacency Matrix}
    \newline
    Let \(M = (m_{i,j})\) be the adjacency matrix of the given emotion graph \(G_e = (V,E)\), where
    \newline
    \begin{equation}
        m_{i,j} = \Bigg\{
        \begin{tabular}{l}
        1  if vertex i and j are linked in the given emotion graph \(G_e\)\\
        0  otherwise
        \end{tabular}
    \end{equation}
    \end{definition}
    \theoremstyle{definition}
    \begin{definition}{Eigenvector Centrality}
    \newline
    For the given emotion graph \(G_e = (V,E)\) and adjacency matrix M = (\(m_{i,j}\)), let \(\lambda\) be a proportionality factor. The eigenvector centrality score \(c_i\) for a vertex \(i\) is calculated as:
    \newline
    \begin{equation}
        \label{eq:eigen}
        c_i = \frac{1}{\lambda}\sum_{j \in V} m_{i,j}c_j
    \end{equation}
    \newline
    Where \(c_j\) is the eigenvector centrality score of any vertex \(j\) adjacency to vertex \(i\).
    With the corresponding eigenvalue \(\lambda\) of matrix \(M\), equation \ref{eq:eigen} can be written in the vector notation:
    \newline
    \begin{equation}
        M \text{c} = \lambda \text{c}
    \end{equation}
    \newline
    Where c is the eigenvector of matrix \(M\).
    \end{definition}
    \theoremstyle{definition}
    \begin{definition}{Clustering Coefficient}
    \newline
    For the given emotion graph \(G_e = (V,E)\) and adjacency matrix M = (\(m_{i,j}\)), the clustering coefficient score \(cl_i\) of vertex \(i\) can be calculated as:
    \newline
    \begin{equation}
        cl_i = \frac{\sum_{i \neq j, j \neq k, i \neq k} m_{i,j} \times m_{j,k} \times m_{i,k}}{\sum_{i \neq j, j \neq k, i \neq k}m_{i,j} \times m_{i,k}} 
    \end{equation}
    Where \(i, j\) and \(k \in V\). The average clustering coefficient score of vertex \(i\) is calculated as:
    \newline
    \begin{equation}
        avgcl_i = \frac{cl_i}{\left| V \right|}
    \end{equation}
    \end{definition}
    There are two kinds of frequent words in a pattern. One is connect words(\(CW\)) and the other is psychology words(\(PW\)).
    \theoremstyle{definition}
    \begin{definition}{Connect Word(\(CW\))}
    \newline
    Let \(CW\) be a set of connect words, \(cw\) is the word with a high eigenvector centrality, $\forall$ \(cw\) $\in$ \(CW\). 
    \end{definition}
    \begin{definition}{Psycology Word(\(PW\))}
    \newline
    Let \(PW\) be a set of psycology words, \(pw\) is the word with a high average clustering coefficient, $\forall$ \(pw\) $\in$ \(PW\).
    \end{definition}
    \par The nodes with high eigenvector centrality tend to be connected word which included stop words. And the nodes with high clustering coefficient tend to be psychology words. By combining at least one cw and one pw word, we can generate the patterns containing stop words.
    \par The frequent word selection process for Chinese text poses an additional difficulty, given that it requires more steps in its re-processing, particularly in word segmentation. In Chinese language, a word can be composed generally by two, three or four characters. Characters on their own may have a meaning when appearing alone, and they can mean something totally different when paired with another character. It is therefore of high importance to perform appropriate character segmentation a and sense disambiguation before proceeding to find the frequent words. In order to segment Chinese characters, the following hierarchical combination is performed:
    \begin{CJK}{UTF8}{bsmi}
    \begin{enumerate}
        \item Combine the characters into two-, three-, and four-character words and calculate their frequency in the dataset.
        \newline
   
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
             & Two-Character Word  & Three-Character Word  & Four-Character Word  \\
            \hline
            Word & 太閒 & 飽太閒 & 吃飽太閒\\
            \hline
            Counts & 300 & 100 & 100 \\
            \hline
        \end{tabular}
        \end{table}
    
        \item Perform an initial reduction on three-character words by subtracting the frequency of four-character words that contain them.
        \newline
   
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
             & Two-Character Word  & Three-Character Word  & Four-Character Word  \\
            \hline
            Word & 太閒 & 飽太閒 & 吃飽太閒\\
            \hline
            Counts & 300 & 0 & 100 \\
            \hline
        \end{tabular}
        \end{table}
        \item Perform an additional reduction on two-character words by subtracting the frequency of three- and four-character words that contain them.
        \newline
   
        \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|c|}
            \hline
             & Two-Character Word  & Three-Character Word  & Four-Character Word  \\
            \hline
            Word & 太閒 & 飽太閒 & 吃飽太閒\\
            \hline
            Counts & 200 & 0 & 100 \\
            \hline
        \end{tabular}
        \end{table}
    \end{enumerate}
    \end{CJK}
    \par After the previous procedure is complete, Chinese frequent words are filtered into two-, three-, and four-character words using an arbitrary frequency threshold. The example of the frequent words is show in Table~\ref{tab:chinese_cw_pw} and Table~\ref{tab:english_cw_pw}.
    \begin{CJK}{UTF8}{bsmi}
    \begin{table}[H]
        \centering
        \begin{tabular}{|p{2cm}|p{9cm}|}
        \hline
        \multicolumn{2}{|c|}{Chinese Word List} \\
            \hline
            PW  & 道歉、浪費、腦殘、可憐的、太誇張、太扯了、欺負弱小、腦袋有洞、無恥之徒  \\
            \hline
            CW & 真是、好了、不如、你們這、給你們、有本事、哈哈哈、有報應的、真她媽的\\
            \hline
        \end{tabular}
        \caption{Examples of connect words and psycology words in Chinese.}
        \label{tab:chinese_cw_pw}
    \end{table}
    \begin{table}[H]
        \centering
        \begin{tabular}{|p{2cm}|p{9cm}|}
        \hline
        \multicolumn{2}{|c|}{English Word List} \\
            \hline
            PW  & glad、disgrace、lucky、appreciate、kidding  \\
            \hline
            CW & you、the、ever、what、such、more\\
            \hline
        \end{tabular}
        \caption{Examples of connect words and psycology words in English.}
        \label{tab:english_cw_pw}
    \end{table}
    \end{CJK}
%\par The framework of our approach have the main three part: Preprocessing, Pattern Selection and Evaluation.
%   Preprocessing is for making sure the Implicit Emotion Patterns containing no any Emotion Word \(EW\), first collected the emotion related words from LIWC and Wordnet. The pattern selection is for extracting the emotion pattern list \(P\), dropping the patterns containing \(EW\) and selecting the Emotion Specific Patterns \(EP\)(ref. Definition 9.).\newline
%   Definition 6: An emotion word \({ew}_i\) is a word collected from the selected categories of LIWC or the synonyms defined in Wordnet.
%    \[  
%     \forall{ew_i} \in \mathcal{EW},\text{words in LIWC and Wordnet}
%    \]
    
%        \begin{figure}[H]
%
%        \includegraphics[scale=0.4]{images/framework.png}
%        \caption{The framework of our approach.}
%        \label{fig:fw}
%    \end{figure}
   
   

    %\newpage
    \subsection{Emotion Patterns}
    \par Finally, we use the extracted \(CW\) and \(PW\) to generate the emotion pattern \(p\). Repetitive instances of sequences in the graph with the length = 2 or length = 3 will become the patterns \(p\).
    \begin{definition}
    The pattern \(p\) is the combination of element \(e_i\) and \(e_j\), where \(e_i\) $\in$ \(CW\) and \(e_j\) $\in$ \(PW\).
    \end{definition}
    %\theoremstyle{definition}
    %\begin{definition}
    %An element \(e\) is a word or a sequence of symbols (,.?!, etc).\newline
    %\end{definition}
    \par Since the graph is filled with subjective expressions, the intuition is that the obtained patterns are expressions that denote a high level of emotion. In order to deal with the misspelling and match more possible pycology words, pw will be changed into wildcard to representing any symbol or character. Table~\ref{tab:extracted_pattern} contains examples of extracted patterns from our training dataset.
    \begin{CJK}{UTF8}{bsmi}
    \begin{table}[H]
        \centering
        \begin{tabular}{|p{2cm}|p{11cm}|}
        \hline
        \multicolumn{2}{|c|}{Pattern Example} \\
            \hline
            Chinese & 你們*、應該要*、他們真的*、*真的很、*有病、*怎麼  \\
            \hline
            Enhlisg & * this is、* for this、the * you、not a *、be * for、your * to \\
            \hline
        \end{tabular}
        \caption{Example of extracted patterns.}
        \label{tab:extracted_pattern}
    \end{table}
    \end{CJK}
    \par It is also important to determine which emotion a pattern is more likely to be expressing. The obtained emotion patterns are then paired to our set of labels through distance supervision. Given that we have our set of patterns and a set of comments with their corresponding emotion labels, the idea is to find how many instances of the patterns are in the corpus. By doing probabilistic analysis, we can come out an Emotion Degree \(ED\) to determine in which particular emotion label a certain pattern was more predominant.
    \begin{definition}
    An Emotion Degree \(ED(emo,p)\) is a score representing how a pattern is related to an specific emotion.
     \[   
    ED(emo,p) \longmapsto ed, ed \in \mathbb{R^+}
    \]
    \end{definition}
    \begin{definition}
    An emotion \(emo\) is defined in a set of 5 emotions.\newline
   All the patterns have 5 Emotion Degree(\(ED\)) for 5 Emotions.
    \[
    emo \in Emotion \{Haha, Angry, Sad, Love, Wow\}
    \]
    \end{definition} 
    As a result, every emotion will contain the same patterns, but ranked in a different order and weighed by Emotion Degree \(ED\) that depends on their frequency, uniqueness, and diversity.
    
    \begin{definition}{Pattern Frequency (PF)}
    \newline
    Pattern Frequency \(PF(emo,p)\) represent the frequency of an emotion pattern \(p\) in an collection of social data related to emotion \(emo\).
    \end{definition}
    \begin{definition}{Inverse Emotion Frequency (IEF)}
    \newline
    The Inverse Emotion Frequency \(IEF(emo,p)\) is a measurement to the importance of the pattern \(p\) across all emotion classes \(emo\).
    \end{definition}
    \begin{definition}{Diversity (DIV)}
    \newline
    Diversity \(DIV(p)\) consider the number of unique psychology words with the pattern \(p\) across all emotion classes \(emo\).
    \end{definition}
    %\begin{itemize}
    %\item Pattern Frequency (PF) : the frequency of the patterns with normalization in each emotion.
    
    %\item Inverse Emotion Frequency (IEF) : similar to the inverse document frequency, it's a measurement to the importance of the pattern across all emotion classes.
    
    %\item Diversity (DIV) : consider the number of unique psychology words with the pattern across all emotion classes. 
    %\end{itemize}
    
    Finally, the emotion degree that shows how important a pattern is in an emotion class is obtained by the equation below.
    \begin{equation}
    %\[
    ED(emo,p) = PF(emo,p) \times IEF(emo,p) \times DIV(p)
    %\]
    \end{equation}
    \par Table~\ref{tab:english_emo_patterns} contains examples of extracted patterns that are ranked high and thus very representative of the corresponding emotions. It is worth noting that the corpus is crawled from Facebook pages of news media, so at the time of the crawl, these were rich in political content, international conflicts, etc. This can be particularly evident for Anger and Sadness, were the topic related to the corresponding comments that generated these patterns can be deduced. Other categories also have particular characteristics. For example, in the Wow emotion, there is a presence of question words such as “what” and “how”, which can be indicators of surprise.
    \begin{table}[H]
    \centering
    \begin{tabular}{|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{3cm}|}
    \hline
    \multicolumn{5}{|c|}{Examples of Patterns by Emotion Reaction}                 \\ \hline
    Angry          & Haha         & Wow         & Sad           & Love             \\ \hline
    * all haters   & * . lol      & * . awesome & * so sad      & * love you       \\
    trump is *     & happy bday * & a * what    & my heart *    & the best *       \\
    what a *       & * ! yeah     & * user omg  & * god bless   & best president * \\
    people are *   & looks so *   & * !!! how   & . rip *       & * !!! thank      \\ \hline
    \end{tabular}
    \caption{Examples of high ranked English patterns for every emotion label.}
    \label{tab:english_emo_patterns}
    \end{table}
    \begin{CJK}{UTF8}{bsmi}
    \par For Chinese Patterns shown in Table~\ref{tab:chinese_emo_patterns}, we can also see the particular characteristics such as English Patterns. In Sad emotion, there is a presence of sad or praying words such as “平安” and “加油”, and in Haha emotion, there is a presence of funny or joy words such as “哈哈哈” and “笑到”.
    \end{CJK}
    \begin{CJK}{UTF8}{bsmi}
    \begin{table}[H]
    \centering
    \begin{tabular}{ |p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
    \hline
    \multicolumn{5}{|c|}{Examples of Chinese Patterns by Emotion Reaction} \\
    \hline Angry & Haha & Wow & Sad & Love \\
    %\hline 不要臉*、出來*、*垃圾& *哈哈哈 可愛* 笑到* & *！！ \quad 覺得*\quad*厲害 & 希望* \quad\quad *加油 \quad *平安& 可愛* \quad\quad 謝謝* \quad *感恩\\
    \hline 
    不要臉* & *哈哈哈 & *！！ & 希望* & 可愛* \\
    出來* & 可愛* & 覺得* & *加油 & 謝謝* \\
    *垃圾 & 笑到* & *厲害 & *平安 & *感恩 \\
    \hline
    \end{tabular}
    \caption{Examples of high ranked Chinese patterns for every emotion label.}
    \label{tab:chinese_emo_patterns}
    \end{table}
    \end{CJK}
    \begin{CJK}{UTF8}{bsmi}
    \par The presence of the in the patterns is also worth noticing. The takes the place of a word that can elicit a high degree of sentiment. These words are replaced by this token so that any word that is in the same way can be matched by these patterns. For instance, the pattern “people are *” could match “people are dumb” or “people are stupid”, both denoting an angry expression, and the pattern “覺得*” could match “覺得不可思議” or “覺得驚訝”, both denoting an wow expression; the usage of the thus allows matching both examples to the same pattern. A pattern’s ability to capture many different instances is what was referred to previously as diversity.
    \end{CJK}
    %\section{Emotion Word}
%\par The first stage in the framework is pre-processing. In this stage, Emotion Word  are been collected from the words in LIWC but also their synonyms defined in Wordnet.
%         We utilize LIWC 2007 version to complete our work. There are 10,557 words collected in this dictionary. And we use the words had been categorised in psychological processes as the picked emotion words. It's near 53\% of the words in the dictionary.\newline
%       
%    
%    \begin{tabular} { |c|c| }\hline 
%        \multirow{Selected categories from LIWC} & affect, postive emotion, negtive \\ 
%            & emotion, anxiety, anger, sad \\  
%        \hline number of of the selected words & 3,419\\
%        \hline number of the expanded words from Wordnet & 8,927 \\\hline
%    \end{tabular}
%    \captionof{table}{The amount of Emotion Words.}%3/22 tabular caption
    
    
    \subsection{Emotion Classification}
    \par The classification process then takes a new unlabeled comment, and through a matrix multiplication procedure, it evaluates it with the patterns and ranks within the labels. The process first determines which patterns are present in the post. It then proceeds to calculate the score of how likely a text is to belong to a class, depending on the score and ranking of the patterns it contains. As a result, the system returns a scored and ranked list of the emotion labels based on the likelihood of the new comment belonging to them. For practical purposes, the top two results are considered as the labels for the evaluated text. These two labels are then evaluated to see if they can provide insights to the presence of sarcasm.
\section{Sarcasm Detection}
    \par Sarcasm is a highly context-dependent reaction---it is usually not planned for, but initially depends on previous information. The post-then-comment scenario from which the data is crawled provides a kind of interaction that may favor this behavior. For example, a user first reads a news post, and depending on his/her opinions towards the topic, s/he might decide to first react to it in a peculiar way and then provide a sarcastic reply to the post.
    
    \par Another characteristic of sarcasm— and one that has troubled the sentiment analysis community—is the reverting of a emotion from the perceptive point of view. This is the typical use of positive statements when actually having a negative point of view. If the receiver is not aware of the state of humor or behavioral traits of the sender, then the message may be perceived as positive, while the intention may have been negative. This poses a significant challenge to automatic emotion classification systems, since they cannot be aware of these particular behavioral traits. Our methodology tries to make use of the flip of emotion as a feature for sarcasm detection, as explained in the following section.

    \subsection{Pattern Score Based Emotion Feature Learning}
    \par Based on the aforementioned user behavior, this stage initially determines if a comment is at all eligible for containing sarcasm. After performing emotion classification for a large set of comments, a particular case arose in which many of the results consisted of opposing sentiment labels. This is perhaps due to the nature of the data and the presence of internet trolls in these kind of sites, which can lead to a user reacting with laughter to a piece of news that would otherwise generate anger in the majority of the population. Nevertheless, this also relates to sarcastic behavior.
    
    \par With this behavior, every short document with opposite emotions considered a candidate for sarcasm detection. To find the combination of emotion which can indicate sarcasm, a machine learning method is performed. Our method tries to make use of combined emotions as a feature for sarcasm detection, as explained in as following.
    
    \begin{itemize}
        \item Convolutional Neural Network
            \begin{itemize}
            \item Input Matrix
            \newline
            Since every emotion will contain same patterns ranked in different order and weighted by a score, we consider the Pattern Scores  \(PS\) of the comment in each emotion as the input matrix of the Convolutional Neural Network.
            
            Every comment evaluated will generate an input matrix as follows:
            \newline
            \begin{center}
            \begin{math}
            \begin{pmatrix} 
            & Pattern Socre_1 & ... & Pattern Score_n\\ 
            Emotion_1 & 300 & ... & 2000\\
            Emotion_2 \\
            Emotion_3 \\
            Emotion_4 \\
            Emotion_5 \\
            \end{pmatrix}
            \newline
            \end{math}
            \end{center}
            
            \par The Pattern Score \(PS\) is the ranking of the pattern in each emotion multiply the frequency of the pattern in the comment .Here we consider n pattern in every emotion where n is experimentally defined.
            
            \item Training Prediction
            \newline
            When training the model, we use the training prediction as our observation source. We then calculate in how many of the iterations is a sarcastic comment correctly identified, in parallel this lets us know those that can be correctly learned by our model. We call this value the correct training rate.
            Since sarcasm has the characteristic of flip of emotion, we then consider the combinations of opposite emotions that allow a correct prediction.
            By calculating the emotion combination results for the range 100\% to 70\% correct training rate, we can extract the specific combinations which represent more precise indicators for sarcastic comments. For example, by observing the correct learning rate at 70\% we can identify which 2 combinations of emotions where more useful for the classification as illustrated by Figure~\ref{fig:learning}:
            \end{itemize}
            % \par Since sarcasm has the characteristic of flip of emotion, we then consider the combinations of opposite emotions in our result.
            % By calculating the emotion combination result from the range 100\% to 60\% training correct rate, the specific combinations which can indicate sarcasm comments can be extracted to do the sarcasm labeling. The example of the result is shown as below:

            \begin{figure}[H]

                \includegraphics[scale=0.45]{images/Eng_cnn.pdf}
                \caption{Example of emotion combination Learning Result for English comments.}
                \label{fig:learning}
            \end{figure}
            %\begin{figure}[H]
            %    \includegraphics[scale=0.45]{images/Chi_cnn.pdf}
            %    \hline
            %    \caption{Chinese emotion combination Learning Result.}
            %\end{figure}
            
    \end{itemize}
    \par After performing a thorough exploration through a set of comments that had this ambiguity (to ensure they were not simply cases of mislabeling), it was observed in many cases that a hint towards sarcastic comments was given. This evaluation was done by reading both the comment and the original post for verification. Every short document with these emotion combination labels is thus considered a candidate for sarcasm detection.
    \subsection{Emotion Feature Based Sarcasm Selection}
    \par Due to the language difference, Comments which has only one significant emotion is very obvious in Chinese language~\cite{quan2010emotion}. But for English, there is around 80\% of the comment has at least two emotion in our observation. So not every comment with same emotion combination are all sarcasm in Chinese, it can has only one specific emotion. 
    \par There is a dependency on the distance between these two initial emotion labels. If the top label is very dominant compared to the subsequent ones there is less chance for it being a sarcastic instance since their is only one significant emotion. On the other hand if the two top labels have similar scores and opposing emotion there is higher chances for it to be a sarcastic comment indicated by the emotion combination.

    \par Once a sarcasm candidate is received, two measurement between its two emotion labels is performed. These value are the determining factor in labeling a comment as sarcastic or not, and is based on Definition~\ref{def:dist_rat} and Definition~\ref{def:score_rat}. With these two measurement, we can labeling a comment as sarcastic or not in Chinese.
    \begin{definition}{Distance Ratio Measurement}
    \label{def:dist_rat}
    \newline
    To make sure there is not only one specific emotion, we measure the difference of emotion score of emotion 1 and 2 with emotion 2 and 3.
    We divide the difference of emotion score of emotion 1 and 2 by emotion 2 and 3.
    The value of our measurement need to greater or equal to x1, and less than or equal to x2 where x1 and x2 are experimentally defined.
    \newline
    \begin{equation}
    x_2 \geq\frac{Score(emotion_3) - Score(emotion_2)}{Score(emotion_2) - Score(emotion_1)} \geq x_1 
    \end{equation}
    \end{definition}
    \begin{definition}{Score Ratio Measurement}
    \label{def:score_rat}
    \newline
    To make sure there is not only one specific emotion, we measure the emotion score ratio of emotion 1 and 2 with emotion 2 and 3.
    The value of emotion score ratio of emotion 1 and 2 need to greater or equal to y1, and the value of emotion score ratio of emotion 2 and 3 need to greater or equal to y2 where y1 and y2 are experimentally defined.
    \newline
    \begin{equation}
    \frac{Score(emotion_3)}{Score(emotion_2)}\geq y_1 , \frac{Score(emotion_2)}{Score(emotion_1)} \geq y_2
    \end{equation}
    \end{definition}


    %\section{Pattern Extraction}\newline
%\par The second stage is pattern extraction. To build the pattern list. Feed the training data set to the Argueta model for getting the oringinal emotion pattern first. The output of this step is the emotion pattern list.\newline
    %Definition 7: An Emotion Pattern \(p_i\) in Emotion Pattern %List \(P\) with any Emotion Word \(ew_i\) is a \(p_{ew}\).
    % \[ \forall{p_i}\in\mathcal{P},\forall{ew_i}\in\mathcal{EW},\forall{p_{ew}}\in\mathcal{P}_{ew}\subset \mathcal{P}\]
    %An Implicit Pattern \(ip_i\) is an emotion pattern \(p_i\) without any emotion word \(ew_i\)
    %\[\mathcal{P-P}_{ew}\mathcal{ = IP} \quad ,\forall{ip_i}\in \mathcal{IP} \subset \mathcal{P}\]
    %\newline
    %\section{emotion specific pattern Selection}\newline
%\par The third stage is pattern selection. To get the patterns which are less explicit and more unique for one emotion. Filter out the \(EW\) collected in the first stage. This process will delete all the patterns containing emotion words from the emotion pattern list. And then filter out the patterns below the threshold setting by the distribution of emotion score for each emotion. And remain those emotion specific patterns which has one and only one emotion score above the threshold and others are below the thresholds. That's how we got the emotion specific pattern list(EP).
    %\newline
    %Definition 8: A threshold of an emotion is a score set as the mean value of the \(ES\) excluded 0 of an emotion.
    % \[
    %thr(emo) = mean(ES_{emo})|, thr(emo) \in \mathbb{R^+}
    %\]
    %Definition 9: An emotion specific pattern \(ep_i\) is an Implicit Pattern \(ip_i\) that has one and only one Emotion Score \(ES(emo,p)\) above the threshold \(thr(emo)\)
    % \[\forall{ep_i} \in
    %\mathcal{EP} \subset IP, \text{A pattern with only 1 out of 6  }ES(emo,p) >thr(emo)
    %\]
    
    %\newpage
    %\section{Data Collecting}\newline
%\par The fourth stage is data collecting. In order to examine how the selected emotion specific patterns performing, crawl the random tweets which containing one of the selected patterns inside. And these are the content avoiding to be appear in the tweets as the data cleaning: re-tweet(tweets contains "RT@"), URL, emoticon, emotion words collected in the first stage and the similar tweets among each other.
    %\newline
    %\section{Evaluation}%\newline
    %\par In the evaluation part, we use two different way to evaluate our method. For the first one, we fulfilled the task by using Amazon Mechanical Turk (AMT), which is a human intelligence platform to asking questionnaires answered by online workers and held by the requester. To examine the performance of the Sarcasm Detection, put the mapped tweets on AMT. We designed a questionnaire shown each tweets, and asked 3 workers to label out do they feel sarcasm in the message.(Figure 3.2) And also the intensity (the scale is 1-5, the higher the intensity is ,the stronger they thought the sarcasm is) of sarcasm they picked. For second one, we ask 3 to 4 native speakers of the corresponding language, and include both males and females between 22 and 30 years of age undergoing graduate studies to labeled the tweets. To assure the answers answered by AMT workers are valid, set the requirement of the worker to the approval rate of the previous assignment they've done to larger than 95\%. To measure how many detected sarcasm tweets have been agreed on sarcasm, we calculate the agreements for 1 person to 3 people agreement. Take 1 person precision as example, if more than 1 answers of the annotated sarcasm is the one we expected for the detected sarcasm tweet, count it as correct. Then compute how many percentage of detected sarcasm tweets are correct. That's 1 person precision. And we do the similar calculation for other precision values.
    %\par In the evaluation part, we fulfilled the task by using Amazon Mechanical Turk (AMT), which is a human intelligence platform to asking questionnaires answered by online workers and held by the requester. To examine the performance of the Emotion Specific Pattern, put the mapped tweets on AMT. We designed a questionnaire shown each tweets containing Emotion Specific Pattern, and asked 6 workers to label out the top two emotion they feel in the message.(Figure 3.2) And also the intensity (the scale is 1-5, the higher the intensity is ,the stronger they thought the emotion is) of the emotion they picked. There are those six emotions and a 'none of above' option inside the options for the situation when they feel the message doesn't portray any suitable emotion. To assure the answers answered by workers are valid, set the requirement of the worker to the approval rate of the previous assignment they've done to larger than 95\%. To measure how many implicit emotion bearing tweets have been agreed on the emotion related to the Implicit Emotion Patterns, we calculate the agreements for 1 person to 6 people agreement. Take 1 person precision as example, if more than 1 answers of the annotated emotion is the one we expected for the implicit emotion specific pattern inside the tweet, count it as correct. Then compute how many percentage of tweets of an emotion as the implicit emotion patterns bearing are correct. That's 1 person precision. And we do the similar calculation for other precision values.
    %要列percision 的公式
%i Person precision_emo=summation(1,n)boolean(emotion of msg_n is the emotion been label out)/n
%for i in 1 to 6, for emo in Emotion set,for n is the number of messages related to the emotion emo.

%\begin{equation}
%    G(x, w) =
%    \begin{cases*}
%      0 & if F(x) \neq w^x_1 and F(x) \neq  w^x_2 \\
%      1 & if F(x) = w^x_1  or F(x) =  w^x_2 \\
%    \end{cases*}
%  \end{equation}
    %\begin{left}
        
    %\begin{figure}[H]
    %    \includegraphics[scale=0.45]{images/Turk.jpg}
    %    \hline
    %    \caption{The design of AMT questionnaire.}
    %\end{figure} \end{left} 
    

% affect, postive emotion, negtive emotion, anxiety, anger, sad.

\chapter{Experiment \& Results}
    
    \section{Experimental Setup}
    \subsection{Data}
        \par A total of 62,248 posts were crawled, together with the comments contained in them. Around 46,253 posts for Chinese with around 3 million comments, and 15995 posts for English with 7 million comments. The posts come from a variety of Facebook Pages belonging to news media outlets in both Chinese and English—both datasets were evaluated separately. After the comments were collected, they were matched with a corresponding reaction chosen by the user. Table~\ref{tab:comment_counts} presents the total counts of comments overlapped to a particular reaction for both English and Chinese datasets. These sets of comments with their self-reported annotations are used to train the system.
        
%\begin{table}[h]
%\centering
%\caption{Counts of collected comments per corresponding emotion for both English and Chinese.}
%\label{tab:counts}
\begin{center}
\begin{tabular}{llllllll}
\cline{1-4}
\multicolumn{1}{|l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Overlapped\\   Emotion\end{tabular}}} & \multicolumn{1}{l|}{\textbf{Chinese Comments}} & \multicolumn{1}{l|}{\textbf{English Comments}} & \multicolumn{1}{l|}{\textbf{Total}} &  &  &  &  \\ \cline{1-4}
\multicolumn{1}{|l|}{Angry}                                                                   & \multicolumn{1}{l|}{167,692}                    & \multicolumn{1}{l|}{206,994}                      & \multicolumn{1}{l|}{374,686}         &  &  &  &  \\
\multicolumn{1}{|l|}{Haha}                                                                    & \multicolumn{1}{l|}{79,444}                     & \multicolumn{1}{l|}{162,149}                      & \multicolumn{1}{l|}{214,593}          &  &  &  &  \\\multicolumn{1}{|l|}{Wow}                                                                     & \multicolumn{1}{l|}{38,433}                     & \multicolumn{1}{l|}{61,720}                      & \multicolumn{1}{l|}{100,153}          &  &  &  &  \\
\multicolumn{1}{|l|}{Sad}                                                                     & \multicolumn{1}{l|}{28,271}                     & \multicolumn{1}{l|}{102,264}                      & \multicolumn{1}{l|}{130,535}          &  &  &  &  \\
\multicolumn{1}{|l|}{Love}                                                                    & \multicolumn{1}{l|}{34,019}                     & \multicolumn{1}{l|}{300,600}                      & \multicolumn{1}{l|}{334,619}          &  &  &  &  \\ \cline{1-4}
\multicolumn{1}{|l|}{\textit{Total}}                                                          & \multicolumn{1}{l|}{347,859}                    & \multicolumn{1}{l|}{833,727}                     & \multicolumn{1}{l|}{1,181,586}         &  &  &  &  \\\cline{1-4}
\\
\end{tabular}
\captionof{table}{Counts of collected comments per corresponding emotion for both English and Chinese.}
\label{tab:comment_counts}
\end{center}

%\end{table}
        
        %\begin{center}
        %    \begin{tabular} { |c|c| }
        %    \hline Categories & Examples\\            
        %    \hline Affective process & happy, cried, abandon\\
        %    \hline Postive emotion & love, nice, sweet\\
        %    \hline Negtive emotion & hurt, ugly, nasty\\
        %    \hline Anxiety & worried, fearful, nervous\\
        %    \hline Anger & hate, kill, annoyed\\
        %    \hline Sad & crying, grief, sad\\
        %    \hline
        %    \end{tabular}
        %\end{center}\captionof{table}{The selected categories from LIWC.}
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.5]{images/Emotion_comment_distribution.pdf}
            %\hline
            \caption{Distribution of comments across emotions for English and Chinese.}
            \label{fig:distribution}
            
        \end{figure}
        \end{center}
        \par The comments in Chinese are in traditional Mandarin characters from predominantly Taiwanese news media. This was the original data set to work with and therefore has a larger volume. The percentage that every reaction represents in the datasets was also calculated. Results for this distribution are presented in Figure~\ref{fig:distribution}. It can be observed that the distribution for both Chinese comments and English comments have similar behaviors, both being dominated by Angry reactions. This kind of distribution can provide some insights on how different groups interact with this kind of media. Furthermore, this can lead to a deeper study on the differences or similarities in interaction based on cultural or language backgrounds.
        \par It is also worth pointing out that the biggest difference is with regards to Sad reactions—where in English it breaks the trend and has a significantly higher percentage than in Chinese. This again may be due to a reflection of the dataset, since the crawling was carried out during a period of presidential elections in the United States. The other most significant difference may concern the Love emotion, where Chinese comments represent a lower percentage. This could be caused by the nature of the original posts, given that western media sometimes seek to promote emotional reactions.
        \par Separate sets of data were crawled at different times to perform evaluation. For evaluation, human annotation was required. The following subsection describes the process for ground truth generation.
    %\section{Ground Truth}
    %    \par As mentioned previously, one of the main challenges in the sarcasm detection task is that it is hard even for humans to identify sarcastic expressions. Therefore, what was meant to be just a ground truth generation also turned out to be a small experiment to determine how well different subjects perceive sarcasm, if there are any cultural differences, and how the results of the annotation can be used as an effective means for evaluation. Sets of 300 unlabeled comments were provided to four different annotators in both Chinese and English. The annotators were native speakers of the corresponding language, and include both males and females between 22 and 30 years of age undergoing graduate studies.
    %    \par The annotation task only requires the annotators to label a comment as being sarcastic or not. Three different levels of agreement were defined from the annotation results. Agreement 1 Label indicates that at least one of the annotators labeled the comment as sarcastic. 2 Label corresponds to two annotators labeling the comment as containing sarcasm and 3 Label indicates agreement between three annotators. A visual representation of the agreement levels for Chinese comments is presented in Figure 6.
    %    \begin{center}
    %    \begin{figure}[H]
    %        \includegraphics[scale=0.5]{images/annotator_chinese.pdf}
            %\hline
    %        \caption{Annotator Agreement for Chinese Comments.
%1 Label indicates the amount of comments
%that where labeled as sarcasm by at least one annotator.
%2 Label and 3 Label correspond to cases where at
%least two and three annotators, respectively, labeled
%the comment as being sarcastic.}
%        \end{figure}
%        \end{center}
    %    \par As expected, the amount of comments label as sarcasm varied from 1 Label to 3 Label in a decreasing order. 33.33\%of comments in Chinese were labeled as sarcastic according to the 1 Label definition. The percentage dropped to 26.53\% for 2 Label and reached a low of 20.41\% for 3 Label. This is visually represented in the figure by the colored lines becoming more scarce as the the amount of annotators to agree increments. This behavior is a clear indication of the difficulty in identifying sarcasm reflected by a decreasing agreement as the number of annotators increases.
    %    \begin{center}
    %    \begin{figure}[H]
    %        \includegraphics[scale=0.5]{images/annotator_english.pdf}
            %\hline
    %        \caption{Annotator Agreement for English Comments.
%1 Label indicates the amount of comments
%that where labeled as sarcasm by at least one annotator.
%2 Label and 3 Label correspond to cases where at
%least two and three annotators, respectively, labeled
%the comment as being sarcastic.}
%        \end{figure}
%        \end{center}
    %    \par In a similar way, Figure 7 presents the agreement levels for the English comments. The most noticeable difference comes from the density of expressions perceived as sarcastic. Up to 77.08\% of the comments evaluated were labeled as sarcasm according to the 1 Label constraints. 2 Label still presented a high percentage at 59.47\%, while 3 Label held a considerable 52.82\%. This means that even when considering three annotators, they all agreed on over half of the sarcasm labels assigned. These values are significantly different than the ones for Chinese comments.
    %    \par The different trends between the percentages for both Chinese and English are presented in Figure 8. The lines have a similar decreasing behavior, which illustrates how, by increasing the number of evaluators, it is hard to reach an agreement when it comes to sarcasm. Nevertheless, the difference between volume is clear across all levels. This might be an indicator of a cultural difference, in which English posting users have a higher tendency to incur in sarcasm usage, or perhaps their use of sarcasm is understood easier by others within the same language group. The obtained annotations with the variation in levels of agreement are all used to evaluate the performance of the classifier for both languages.
    

    %\section{Evaluation}
    %    \par The sets of comments that were used for annotation were processed in parallel by the automatic detection system. After performing the evaluation, the system simply assigns a label of sarcasm or no sarcasm to every processed comment. For each of the ground truths, a result is generated whenever there is a match between the system and the annotation label. Agree 1 corresponds to a match between the system’s result and 1 Label annotation, Agree 2 and 3 correspond to 2 and 3 Label, respectively. The performance metrics considered are Precision, Recall and F-Measure. Figure 9 presents the sarcasm detection performance results for both the Chinese and English sets.
    %    \par It is clear that the system design leans towards better performance in terms of precision. At the time of evaluation, there were no other methods or algorithms that matched the data characteristics to perform a performance comparison. It is positive to see the performance behaving in a similar way for both sets of languages, which indicates that the method’s components are indeed suitable for multi-lingual implementations.
    %    \par The under-performance in terms of recall is a reflection of the previously mentioned issues. Firstly, there is a lack of a more conclusive ground truth given the difficulties posed during annotation. Secondly, and in a similar way to humans, it is a challenge for a trained system to find all instances of sarcasm. Additionally, the contextual knowledge of the human annotator, who may be familiar with a particular topic, may aid in understanding a comment and determine if it is sarcastic or not. For the system, on the other hand, there is no context to leverage, this perhaps limits its ability to exhaustively identify the sarcastic texts and have a higher recall.
    \subsection{Evaluation method}
    \par As mentioned previously, one of the main challenges in the sarcasm detection task is the difficulty even for humans to identify sarcastic expressions. It is required to have annotated testing sets of good quality and consistency in order to perform a proper evaluation. We therefore generated several testing sets, 4 for English evaluation and 3 for Chinese. The inter-annotator consistency of out datasets is measured by Fleiss' Kappa. A good inter-annotator score guarantees the quality of the testing sets, while maintaining this quality across sets is an indicator of consistency. The details of the testing sets are provided in Table~\ref{tab:testing}.
    \par The annotation task requires the annotators to label a comment as being sarcastic or not. The comments for the English sets are a combination of different platforms and contain Facebook comments as well as Tweets from Twitter. The texts are collected randomly from several time periods to avoid any particular bias to a specific news or season. The comments for the Chinese testing sets are collected only from Facebook comments, but again being posted in different articles and time periods randomly selected. The annotators for the English and Chinese Test 1, 2 ,3 sets are university students between 22 and 29 years old, native speakers of the language being evaluated. The subjects are familiar to the social media platforms and the sarcastic posting behavior in it. Not all annotators evaluate all of the sets as observed in the ”\#of Annotators” column in Table~\ref{tab:testing}, different combinations of annotators worked on different sets but as observed they maintain good Fleiss' Kappa scores. According to the suggested interpretation all sets achieve at least Substantial agreement(0.61-0.80).
    \renewcommand\tabcolsep{2pt}
\begin{table}[H]
\centering
\begin{tabular}{l|c|c|c|c}
\hline
{\textbf{Language}}                                      & \multicolumn{1}{c|}{\textbf{Set}} & \multicolumn{1}{l|}{\textbf{\# of Texts}} & \multicolumn{1}{l|}{\textbf{\# of Annotators}} & \multicolumn{1}{l}{\textbf{Fleiss' Kappa}} \\ \hline
\multicolumn{1}{c|}{\textbf{English}} & Test 1                           & 430                                 & 3                                     & 0.7426                            \\
\multicolumn{1}{c|}{}                         & Test 2                           & 260                                 & 4                                     & 0.7391                            \\
\multicolumn{1}{c|}{}                         & Test 3                           & 400                                 & 4                                     & 0.7563                            \\
\multicolumn{1}{c|}{}                         & Test Turk                        & 720                                 & 3                                     & 0.7148                            \\ \hline
\textbf{Chinese}                      & Test 1                           & 300                                 & 5                                     & 0.8560                            \\
                                              & Test 2                           & 294                                 & 6                                     & 0.9104                            \\
                                              & Test 3                           & 346                                 & 6                                     & 0.7630                            \\ \hline
\end{tabular}
\caption{Details of the testing sets to be used for evaluation.}
\label{tab:testing}
\end{table}
    \par The Test Turk set comes from a task submitted to Amazon Mechanical Turk (AMT), which is a human intelligence platform to asking questionnaires answered by online workers and held by the requester. Every text in Test Turk set was rated by 3 annotators who's approval rate of the previous assignment they've done is larger than 95\%. Additionally they were asked to provide a degree of intensity which is not used in this work but might come useful in the future. The design of the questionnaire is shown in Figure~\ref{fig:AMT}. The task contained a few manually inserted comments regarded as definitely sarcastic and definitely not sarcastic to verify if the annotators could perform the evaluation correctly.

    %\par In the evaluation part, we use two different way to evaluate our method. For the first one, we fulfilled the task by using Amazon Mechanical Turk (AMT), which is a human intelligence platform to asking questionnaires answered by online workers and held by the requester. To examine the performance of the Sarcasm Detection, put the mapped tweets on AMT. We designed a questionnaire shown each tweets, and asked 3 workers to label out do they feel sarcasm in the message.(Figure 3.2) And also the intensity (the scale is 1-5, the higher the intensity is ,the stronger they thought the sarcasm is) of sarcasm they picked. For second one, we ask 3 to 4 native speakers of the corresponding language, and include both males and females between 22 and 30 years of age undergoing graduate studies to labeled the tweets. To assure the answers answered by AMT workers are valid, set the requirement of the worker to the approval rate of the previous assignment they've done to larger than 95\%. To measure how many detected sarcasm tweets have been agreed on sarcasm, we calculate the agreements for 1 person to 3 people agreement. Take 1 person precision as example, if more than 1 answers of the annotated sarcasm is the one we expected for the detected sarcasm tweet, count it as correct. Then compute how many percentage of detected sarcasm tweets are correct. That's 1 person precision. And we do the similar calculation for other precision values.
    
    \begin{figure}[H]
        \includegraphics[scale=0.45]{images/Turk.jpg}
        \caption{The design of AMT questionnaire.}
        \label{fig:AMT}
    \end{figure} 
     
    \subsection{Experiments}
    \begin{itemize}
        \item English Sarcasm Detection
        \begin{itemize}
            \item Pattern Score Based Emotion Feature Learning
            \item English Sarcasm Detection Performance Comparison
            %\item 300, 200, 310 comments for testing separately
            %\item 500 comments for Amazon Turk testing
        \end{itemize}
        \item Cross language approach – Chinese language Detection
        \begin{itemize}
            \item Pattern Score Based Emotion Feature Learning
            \item Chinese Sarcasm Detection Performance
            \item Sarcasm Detection Evaluation with Context
            %\item 300, 350, 300 comments for testing separately
        \end{itemize}
        % \item Ask 3 to 4 native speakers of the corresponding language to label.
        % \item Amazon Turk testing –
        % \begin{itemize}
        %     \item Ask 3 annotators on AMT to annotate sarcasm for each comment.
        %     \item Every annotators satisfy the approval rate to above 95%.
        % \end{itemize}
    \end{itemize}
    \section{English Sarcasm Detection Result}
        \subsection{Emotion Feature Learning Result of English Sarcasm}
        \par Calculation of correctly predicted times during the training process is illustrated in the figure~\ref{fig:sar_learning_english}.
        The correctly predicted times are calculated from 100\% correct to 70\% correct. The emotion combination of emotion Angry \& Haha and Angry \& Wow are predicted more correctly. Hence, we choose these two combination for English sarcasm labeling.
        
        %Here is the result for English sarcasm learning
        %The correctly predicted times are calculated from 100\% correct to 70\% correct.
        %We can see the emotion combination of emotion Angry & Haha and Angry & Wow are predicted more correctly.
        %Hence, we choose these two combination for English sarcasm labeling.
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/Eng_cnn2.pdf}
            %\hline
            \caption{Sarcasm Learning Result of English Comments.}
            \label{fig:sar_learning_english}
        \end{figure}
        \end{center}
        \subsection{English Sarcasm Detection Performance}
        \par To evaluate the performance of our Sarcasm
        Classifier for English texts three comparison methods were implemented. The first comparison method is an implementation of the Parsing-Based Lexicon Generation Algorithm(PBLGA) method developed by Bharti et al.~\cite{bharti2015parsing}. This method was trained with short documents containing the hashtag \#sarcasm as indicated by the referenced work.
        \par The second and third comparison method are text classification baselines trained with a corpus related to the topic at hand using TF-IDF and Bag of Words(BOW) based features respectively to train Naive Bayes classifiers. The method introduced in this work will be referred to as Emo-Based in the results to be presented. All four English test sets were processed by the four classifiers mentioned in the previous paragraph. Tree different levels of agreement are considered to determine the correctness of a classification. Agree 1 means that the output label of the classifier matched the label of at least 1 annotator. Agree 2 requires the classifier to match the label of at least 2 human annotators. Subsequently Agree 3 means at least 3 annotators agree to a label and so does the classifier.
        %\par In the experiment part, we compare with three different approach, which is PBLGA approach, TFIDF approach, and Bag of Words approach. For testing, there are four different testing dataset to evaluate the performance.
        % \par For PBLGA approach, we trained with three different training dataset in order to do the fair comparison not only with our own dataset but also consider the refined dataset from the previous work. The first dataset is the Regular training datatset which contain 40,000 regular 
        % \par The second and third dataset are the Refined training datatset which contain 3000 and 5000 refined sarcasm comments from the previous work~\cite{ashok2017sarcasm} separately which is filtered after crawling with sarcasm indicator. 
        \par For PBLGA approach, we trained with 37,000 sarcasm comments which is crawled by sarcasm indicator.
        After we trained the PBLGA model, we tested with each testing data. 
        % The AUC evaluation result in shown in Table~\ref{tab:auc_agree}.
        The results are shown in Table~\ref{tab:pblga_agree1},Table~\ref{tab:pblga_agree2}, and Table~\ref{tab:pblga_agree3}.
        % \renewcommand{\arraystretch}{0.5}
        % \begin{table}[H]
    
        % \centering

        % \small
        % \begin{tabular}{ll|r|r|r}
        % \hline
        % \multicolumn{5}{c}{\textbf{AUC value}}\\
        % \hline
        % \multicolumn{1}{c|}{Set} & \multicolumn{1}{c|}{Method} & Agree 1 & Agree 2 & Agree 3\\
        % \hline
        % \multicolumn{1}{l|}{\textbf{Test 1}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.23\%}} & \textit{\textbf{57.39\%}} & \textit{\textbf{58.43\%}}\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{PBLGA}} & 49.53\% & 50.62\% & 50.14\%\\
        
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test 2}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.32\%}} & \textit{\textbf{60.30\%}} & \textit{\textbf{51.15\%}}\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{PBLGA}} & 50.29\% & 51.91\% & 50.27\%\\
        
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test 3}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.59\%}} & \textit{\textbf{56.36\%}} & \textit{\textbf{50.37\%}}\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{PBLGA}} & 52.09\% & 50.77\% & 49.21\%\\
        
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test Turk}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{65.27\%}} & \textit{\textbf{59.81\%}} & \textit{\textbf{56.04\%}}\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{PBLGA}} & 50.97\% & 52.12\% & 52.38\%\\
        
        % \hline
        % \end{tabular}
        % \caption{AUC score comparison with pblga on all agreement in different dataset.}
        % \label{tab:auc_agree}
        % \end{table}
        
        % Here we can see that our Emo-Based method and PBLGA approach methods in most of the situation has the AUC value greater than 0.5 which indicate that both method has their value in classification.
        % For our Emo-Based method, we can have better AUC value on all agreement in different dataset.
        % In order to have more  comparison information, we then get into a more detail analysis in Accuracy, F1-measurement, Recall, and Precision.
        % The results are shown in Table~\ref{tab:pblga_agree1},Table~\ref{tab:pblga_agree2}, and Table~\ref{tab:pblga_agree3}.
        % \newline
        %\setlength{\tabcolsep}{2pt}
        \renewcommand{\arraystretch}{0.5}
        \begin{table}[H]
            \centering
            \begin{tabular}{l | | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 1}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & \textit{\textbf{60.23\%}} & \textit{\textbf{60.23\%}} & \textit{\textbf{48.95\%}} & \textit{\textbf{38.13\%}} & \textit{\textbf{68.33\%}}\\
                 & PBLGA & 49.53\% & 49.53\% & 12.85\% & 7.44\% & 47.05\%\\
                
                \hline
                Test 2 & Emo-Based & \textit{\textbf{60.32\%}} & \textit{\textbf{60.00\%}} & \textit{\textbf{50.00\%}} & \textit{\textbf{39.39\%}} & \textit{\textbf{68.42\%}}\\
                 & PBLGA & 50.29\% & 49.61\% & 10.88\% & 6.06\% & 53.33\%\\
                
                \hline
                Test 3 & Emo-Based & \textit{\textbf{60.59\%}} & \textit{\textbf{60.75\%}} & \textit{\textbf{53.41\%}} & \textit{\textbf{45.45\%}} & \textit{\textbf{64.74\%}}\\
                 & PBLGA & 52.09\% & 52.50\% & 18.80\% & 11.11\% & 61.11\%\\
                
                \hline
                Test Turk & Emo-Based & \textit{\textbf{65.27\%}} & \textit{\textbf{65.27\%}} & \textit{\textbf{56.14\%}} & \textit{\textbf{44.44\%}} & \textit{\textbf{79.16\%}}\\
                 & PBLGA & 50.97\% & 50.97\% & 16.94\% & 10.00\% & 55.38\%\\
                
                \hline
            \end{tabular}
            \caption{Performance comparison with PBLGA on 1 people agreement.}
            \label{tab:pblga_agree1}
        \end{table}
        \begin{table}[H]
            \centering
            \begin{tabular}{l | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 2}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & \textit{\textbf{57.39\%}} & 65.34\% & \textit{\textbf{44.94\%}} & \textit{\textbf{38.40\%}} & \textit{\textbf{54.16\%}}\\
                 & PBLGA & 50.62\% & 68.13\% & 14.40\% & 8.80\% & 39.70\%\\
                
                \hline
                Test 2 & Emo-Based & \textit{\textbf{60.30\%}} & 67.69\% & \textit{\textbf{48.90\%}} & \textit{\textbf{44.28\%}} & \textit{\textbf{54.60\%}}\\
                 & PBLGA & 51.91\% & 71.92\% & 14.48\% & 8.57\% & 46.66\%\\
                
                \hline
                Test 3 & Emo-Based & \textit{\textbf{56.36\%}} & 61.50\% & \textit{\textbf{47.10\%}} & \textit{\textbf{43.69\%}} & \textit{\textbf{51.07\%}}\\
                 & PBLGA & 50.77\% & 67.25\% & 16.61\% & 10.08\% & 47.22\%\\
                
                \hline
                Test Turk & Emo-Based & \textit{\textbf{59.81\%}} & 67.77\% & \textit{\textbf{49.55\%}} & \textit{\textbf{43.88\%}} & \textit{\textbf{56.90\%}}\\
                 & PBLGA & 52.12\% & 72.08\% & 19.18\% & 12.22\% & 44.61\%\\
                
                \hline
            \end{tabular}
            \caption{Performance comparison with PBLGA on 2 people agreement.}
            \label{tab:pblga_agree2}
        \end{table}
        \begin{table}[H]
            \centering
            \begin{tabular}{l | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 3}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & \textit{\textbf{58.43\%}} & 70.46\% & \textit{\textbf{42.27\%}} & \textit{\textbf{42.85\%}} & \textit{\textbf{41.70\%}}\\
                 & PBLGA & 50.14\% & 82.55\% & 12.85\% & 8.16\% & 30.20\%\\
                
                \hline
                Test 2 & Emo-Based & \textit{\textbf{51.15\%}} & 66.15\% & \textit{\textbf{35.28\%}} & \textit{\textbf{31.25\%}} & \textit{\textbf{40.51\%}}\\
                 & PBLGA & 50.27\% & 83.46\% & 10.62\% & 6.25\% & 35.33\%\\
                
                \hline
                Test 3 & Emo-Based & \textit{\textbf{50.37\%}} & 60.50\% & \textit{\textbf{37.25\%}} & \textit{\textbf{35.38\%}} & \textit{\textbf{39.33\%}}\\
                 & PBLGA & 49.21\% & 77.25\% & 12.66\% & 7.69\% & 35.88\%\\
                
                \hline
                Test Turk & Emo-Based & \textit{\textbf{56.04\%}} & 69.44\% & \textit{\textbf{40.66\%}} & \textit{\textbf{40.38\%}} & \textit{\textbf{40.95\%}}\\
                 & PBLGA & 52.38\% & 85.69\% & 19.14\% & 13.46\% & 33.10\%\\
                
                \hline
            \end{tabular}
            \caption{Performance comparison with PBLGA on 3 people agreement.}
            \label{tab:pblga_agree3}
        \end{table}
            
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Compare_PBLGA12.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Performance Comparison with PBLGA.}
        % \end{figure}
        % \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Compare_PBLGA13.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Performance Comparison with PBLGA.}
        % \end{figure}
        % \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Compare_PBLGA14.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Performance Comparison with PBLGA.}
        % \end{figure}
        % \end{center}
        \par Here we can see that our Emo-Based method and PBLGA approach methods in most of the situation has the AUC value greater than 0.5 which indicate that both method has their value in classification.
        For our Emo-Based method, we can have better AUC value on all agreement in different dataset.
        In order to have more  comparison information, we then get into a more detail analysis in Accuracy, F1-measurement, Recall, and Precision.
        \par As we can see, our Emo-Based method perform much better then PBLGA method in all the evaluation values on Agree 1. But the accuracy improves when it goes to Agree 2 and Agree 3 which appeared surprising. After taking a look at the classification output of PBLGA it was found that opposite from the content based methods, PBLGA is very selective on where to label sarcasm, therefore the fewer cases of sarcasm present in the ground truth, the better the accuracy since it will classify most of the non-sarcasm correctly, nevertheless it suffers alot in recall. For our Emo-Based method, although it doesn't perform as well as PBLGA in Accuracy, it still perform better then PBLGA method on Precision, Recall, and F1-Score which indicate a better result when the ground truth gets more strict. The Emo-Based sarcasm classifier receives emotions from a pattern based approach which can provide more context.
        
        % \par For Tfidf and Bag-of-word approaches, we also trained with three different training dataset to do the fair comparison. The first dataset is the Regular training datatset which contain 1586 regular data which is compose of half of sarcasm and normal comments, but only crawled by sarcasm indicator. 
        % \par The second and third dataset are the Refined training datatset which contain 2676 and 2390 refined data from the previous work~\cite{ashok2017sarcasm} which is compose of half of sarcasm and normal comments separately, and filtered after crawling with sarcasm indicator.
        \par For Tfidf and Bag-of-word approaches, we trained with the data which is compose of 37,000 sarcasm contents crawled by sarcasm indicator and 37,000 normal contents crawled from factual news which is non-emotional.
        After we trained the Tfidf and Bag-of-word model, we tested with each testing data. 
        % The AUC evaluation result in shown in Table~\ref{tab:auc_agree_2}.
        The results are shown in Table~\ref{tab:tfidf_bow_agree1}, Table~\ref{tab:tfidf_bow_agree2}, and Table~\ref{tab:tfidf_bow_agree3}.
        % \renewcommand{\arraystretch}{0.5}
        % \begin{table}[H]
    
        % \centering

        % \small
        % \begin{tabular}{ll|r|r|r}
        % \hline
        % \multicolumn{5}{c}{\textbf{AUC value}}\\
        % \hline
        % \multicolumn{1}{c|}{Set} & \multicolumn{1}{c|}{Method} & Agree 1 & Agree 2 & Agree 3\\
        % \hline
        % \multicolumn{1}{l|}{\textbf{Test 1}} & \textit{\textbf{Emo-Based}} & 60.23\% & 57.39\% & 58.43\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{Tfidf}} & 64.65\% & 61.69\% & 60.99\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{BOW}} & 64.18\% & 62.68\% & 60.94\%\\
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test 2}} & \textit{\textbf{Emo-Based}} & 60.32\% & 60.30\% & 51.15\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{Tfidf}} & 65.37\% & 64.13\% & 62.03\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{BOW}} & 66.51\% & 62.36\% & 61.37\%\\
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test 3}} & \textit{\textbf{Emo-Based}} & 60.59\% & 56.36\% & 50.37\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{Tfidf}} & 59.47\% & 56.86\% & 54.70\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{BOW}} & 62.00\% & 58.91\% & 55.67\%\\
        % \hline
        
        % \multicolumn{1}{l|}{\textbf{Test Turk}} & \textit{\textbf{Emo-Based}} & 65.27\% & 59.81\% & 56.04\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{Tfidf}} & 67.22\% & 62.59\% & 62.16\%\\
        
        % \multicolumn{1}{l|}{} & \textit{\textbf{BOW}} & 68.19\% & 63.79\% & 62.51\%\\
        % \hline
        % \end{tabular}
        % \caption{AUC score comparison with tfidf and bow on all agreement in different dataset.}
        % \label{tab:auc_agree_2}
        % \end{table}
        % Here we can see that our Emo-Based method and the two comparison methods all has the AUC value greater than 0.5 which indicate that all the three method has their value in classification.
        % Different from the comparison with PBLGA approach, Tfidf and Bag-of-word approach has higher AUC value than us. Although we can not have higher AUC value, our Emo-Based method still can performance close to them in this overall evaluation metric since we only based on the emotions in the contents.
        % In order to have more  comparison information, we then get into a more detail analysis in Accuracy, F1-measurement, Recall, and Precision. 
        % The results are shown in Table~\ref{tab:tfidf_bow_agree1}, Table~\ref{tab:tfidf_bow_agree2}, and Table~\ref{tab:tfidf_bow_agree3}.
        \begin{table}[H]
            \centering
            \begin{tabular}{l | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 1}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & 60.23\% & 60.23\% & 48.95\% & 38.13\% & \textit{\textbf{68.33\%}}\\
                 & TFIDF & 64.65\% & 64.65\% & 63.10\% & 60.46\% & 65.98\%\\
                 & BOW & 64.18\% & 64.18\% & 64.18\% & 64.18\% & 64.18\%\\
                \hline
                Test 2 & Emo-Based & 60.32\% & 60.00\% & 50.00\% & 39.39\% & \textit{\textbf{68.42\%}}\\
                 & TFIDF & 65.37\% & 65.38\% & 65.90\% & 65.90\% & 65.90\%\\
                 & BOW & 66.51\% & 66.53\% & 67.41\% & 68.18\% & 66.66\%\\
                \hline
                Test 3 & Emo-Based & 60.59\% & 60.75\% & 53.41\% & 45.45\% & \textit{\textbf{64.74\%}}\\
                 & TFIDF & 59.47\% & 59.50\% & 58.24\% & 57.07\% & 59.47\%\\
                 & BOW & 62.00\% & 62.00\% & 62.00\% & 62.26\% & 61.38\%\\
                \hline
                Test Turk & Emo-Based & 65.27\% & 65.27\% & 56.14\% & 44.44\% & \textit{\textbf{76.19\%}}\\
                 & TFIDF & 67.22\% & 67.22\% & 66.09\% & 63.88\% & 68.45\%\\
                 & BOW & 68.19\% & 68.19\% & 68.15\% & 68.05\% & 68.24\%\\
                \hline
            \end{tabular}
            \caption{Performance comparison with tfidf and bow on 1 people agreement.}
            \label{tab:tfidf_bow_agree1}
        \end{table}
        \begin{table}[H]
            \centering
            \begin{tabular}{l | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 2}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & 57.39\% & \textit{\textbf{65.34\%}} & 44.94\% & 38.40\% & \textit{\textbf{54.16\%}}\\
                 & TFIDF & 61.69\% & 61.39\% & 57.19\% & 62.40\% & 52.79\%\\
                 & BOW & 62.68\% & 60.46\% & 58.84\% & 68.00\% & 51.86\%\\
                \hline
                Test 2 & Emo-Based & 60.30\% & \textit{\textbf{67.69\%}} & 48.90\% & 44.28\% & \textit{\textbf{54.60\%}}\\
                 & TFIDF & 64.13\% & 60.76\% & 60.11\% & 71.42\% & 51.89\%\\
                 & BOW & 62.36\% & 58.84\% & 59.32\% & 70.00\% & 51.48\%\\
                \hline
                Test 3 & Emo-Based & 56.36\% & \textit{\textbf{61.50\%}} & 47.10\% & 43.69\% & \textit{\textbf{51.07\%}}\\
                 & TFIDF & 56.86\% & 56.75\% & 51.95\% & 57.14\% & 47.63\%\\
                 & BOW & 58.91\% & 57.25\% & 55.29\% & 63.02\% & 49.25\%\\
                \hline
                Test Turk & Emo-Based & 59.81\% & \textit{\textbf{67.77\%}} & 49.55\% & 43.88\% & \textit{\textbf{56.90\%}}\\
                 & TFIDF & 62.59\% & 61.11\% & 57.86\% & 65.55\% & 51.78\%\\
                 & BOW & 63.97\% & 60.41\% & 59.74\% & 70.55\% & 51.81\%\\
                \hline
            \end{tabular}
            \caption{Performance comparison with tfidf and bow on 2 people agreement.}
            \label{tab:tfidf_bow_agree2}
        \end{table}
        \begin{table}[H]
            \centering
            \begin{tabular}{l | l | l | l | l | l | l}
                \hline
                \multicolumn{7}{c}{\textbf{Agree 3}} \\
                \hline
                Set & Method & AUC & Accuracy & F1 & Recall & Precision \\
                \hline \hline
                Test 1 & Emo-Based & 58.43\% & \textit{\textbf{70.46\%}} & 42.27\% & 42.85\% & \textit{\textbf{41.70\%}}\\
                 & TFIDF & 60.99\% & 57.67\% & 49.89\% & 65.30\% & 40.36\%\\
                 & BOW & 60.94\% & 54.41\% & 50.42\% & 69.38\% & 39.60\%\\
                \hline
                Test 2 & Emo-Based & 51.15\% & \textit{\textbf{66.15\%}} & 35.28\% & 31.25\% &\textit{\textbf{40.51\%}}\\
                 & TFIDF & 62.03\% & 54.61\% & 51.54\% & 71.87\% & 40.17\%\\
                 & BOW & 61.37\% & 53.46\% & 51.20\% & 71.87\% & 39.77\%\\
                \hline
                Test 3 & Emo-Based & 50.37\% & \textit{\textbf{60.50\%}} & 37.25\% & 35.38\% & \textit{\textbf{39.33\%}}\\
                 & TFIDF & 54.70\% & 54.25\% & 44.98\% & 55.38\% & 37.87\%\\
                 & BOW & 55.67\% & 52.75\% & 47.32\% & 60.00\% & 39.07\%\\
                \hline
                Test Turk & Emo-Based & 56.04\% & \textit{\textbf{69.44\%}} & 40.66\% & 40.38\% & \textit{\textbf{40.95\%}}\\
                 & TFIDF & 62.16\% & 56.11\% & 48.91\% & 69.23\% & 37.82\%\\
                 & BOW & 62.51\% & 53.47\% & 49.82\% & 73.07\% & 37.79\%\\
                \hline
            \end{tabular}
            \caption{Performance comparison with tfidf and bow on 3 people agreement.}
            \label{tab:tfidf_bow_agree3}
        \end{table}
        \par Here we can see that our Emo-Based method and the two comparison methods all has the AUC value greater than 0.5 which indicate that all the three method has their value in classification.
        Different from the comparison with PBLGA approach, Tfidf and Bag-of-word approach has higher AUC value than us. Although we can not have higher AUC value, our Emo-Based method still can performance close to them in this overall evaluation metric since we only based on the emotions in the contents.
        In order to have more  comparison information, we then get into a more detail analysis in Accuracy, F1-measurement, Recall, and Precision.
        \par As shown in the tables, our Emo-Based method perform much better then TF-IDF and BOW method in Precision on all the three Agree.
        For Accuracy, it is a little lower in Agree 1 but perform better in Agree 2 and Agree 3.
        On the other hand, TF-IDF and BOW methods perform better in Recall and F1-Score on all the three evaluation since they are content based methods and the first level of agreement contains more texts labeled as sarcasm. But on 2 Agree and 3 Agree, their performance is affected as the ground truth gets more strict. Their Accuracy, Precision and F1-Score become lower than previous since the second and third level of agreement contains less texts labeled as sarcasm. After taking a look at the classification by these methods we found they are generous in assigning a sarcasm label since it its determined by the presence of specific terms learned from sarcasm contents but doesn't consider context, this might also cause the mis-classification problem on sarcasm contents. On the other hand, our approach is more convenient since we do not base on any sarcasm terms learned from sarcasm contents, we only use the emotions extracted from the contents which is not directly related to sarcasm to do the classification.
        This is also the reason that they can have better performance in Recall and F1 score, since their Accuracy and Precision can not have good performance in the strict Agreement evaluation.
        Although the AUC value of our method can not be as good as Tfidf and Bag-of-word approach, we still can see that our Emo-Based method perform more stable and indicate a balanced classifier between different compare method in different  dataset.
        \par Another advantage is that since our Emo-Base approach use emotions as features, the emotion is expressed by the users, we can find more insight on the user behavior.
        Meanwhile tfidf and bow only focus on frequency based text value.
        Not only through the result, through the evolution we can see how different emotion interact together, meanwhile if we only limit ourselves to text classifiers, we can not really to understand or get the meaning out of the data. It is more easier for us to have more detail information since we use emotion as our features.
        % \par By using the behavior insights from the emotions, we can understand how user interact in different ways which can make our approach apply to other domain such as fake news analysis, since that their is no direct indicators so there may have difficulty for the traditional text based approaches like tfidf and bow. For our Emo-Based approach, we can have more possibility to find the behavior insights between user comments and reactions and find different kinds of expressions related to emotions to do the analysis. 
        % serious thing in joking way with original post.
        
%\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1}
\begin{table}[H]
    
\centering

\scriptsize
\begin{tabular}{ll|rrrr|rrrr|rrrr}
\hline
                                                      &                             & \multicolumn{4}{c|}{\textbf{Agree 1}}                                                                                                                   & \multicolumn{4}{c|}{\textbf{Agree 2}}                                                                                                                   & \multicolumn{4}{c}{\textbf{Agree 3}}                                                                                                                   \\ \hline
\multicolumn{1}{c|}{Set}                              & \multicolumn{1}{c|}{Method} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c}{\textit{Precision}} \\ \hline
\multicolumn{1}{l|}{\textbf{Test 1}} & \textit{\textbf{Emo-Based}} & 60.23\% & 48.95\% & 38.13\% & \textit{\textbf{68.33\%}} & 65.34\% & 44.94\% & 38.40\% & \textit{\textbf{54.16\%}} & 70.46\% & 42.27\% & 42.85\% & \textit{\textbf{41.70\%}}\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.53\% & 12.85\% & 7.44\% & 47.05\% & 68.13\% & 14.40\% & 8.80\% & 39.70\% & 82.55\% & 12.85\% & 8.16\% & 30.20\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 64.65\% & 63.10\% & 60.46\% & 65.98\% & 61.39\% & 57.19\% & 62.40\% & 52.79\% & 57.67\% & 49.89\% & 65.30\% & 40.36\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 64.18\% & 64.18\% & 64.18\% & 64.18\% & 60.46\% & 58.84\% & 68.00\% & 51.86\% & 54.41\% & 50.42\% & 69.38\% & 39.60\%\\ \hline
\multicolumn{1}{l|}{\textbf{Test 2}} & \textit{\textbf{Emo-Based}} & 60.00\% & 50.00\% & 39.39\% & \textit{\textbf{68.42\%}} & 67.69\% & 48.90\% & 44.28\% & \textit{\textbf{54.60\%}} & 66.15\% & 35.28\% & 31.25\% & \textit{\textbf{40.51\%}}\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.61\% & 10.88\% & 6.06\% & 53.33\% & 71.92\% & 14.48\% & 8.57\% & 46.66\% & 83.46\% & 10.62\% & 6.25\% & 35.33\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 65.38\% & 65.90\% & 65.90\% & 65.90\% & 60.76\% & 60.11\% & 71.42\% & 51.89\% & 54.61\% & 51.54\% & 71.87\% & 40.17\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 66.53\% & 67.41\% & 68.18\% & 66.66\% & 58.84\% & 59.32\% & 70.00\% & 51.48\% & 53.46\% & 51.20\% & 71.87\% & 39.77\%\\ \hline
\multicolumn{1}{l|}{\textbf{Test 3}} & \textit{\textbf{Emo-Based}} & 60.75\% & 53.41\% & 45.45\% & \textit{\textbf{64.74\%}} & 61.50\% & 47.10\% & 43.69\% & \textit{\textbf{51.07\%}} & 60.50\% & 37.25\% & 35.38\% & \textit{\textbf{39.33\%}}\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 52.50\% & 18.80\% & 11.11\% & 61.11\% & 67.25\% & 16.61\% & 10.08\% & 47.22\% & 77.25\% & 12.66\% & 7.69\% & 35.88\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 59.50\% & 58.24\% & 57.07\% & 59.47\% & 56.75\% & 51.95\% & 57.14\% & 47.63\% & 54.25\% & 44.98\% & 55.38\% & 37.87\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 62.00\% & 62.00\% & 62.26\% & 61.38\% & 57.25\% & 55.29\% & 63.02\% & 49.25\% & 52.75\% & 47.32\% & 60.00\% & 39.07\%\\ \hline
\multicolumn{1}{l|}{\textbf{Turk}}   & \textit{\textbf{Emo-Based}} & 65.27\% & 56.14\% & 44.44\% & \textit{\textbf{79.16\%}} & 67.77\% & 49.55\% & 43.88\% & \textit{\textbf{56.90\%}} & 69.44\% & 40.66\% & 40.38\% & \textit{\textbf{40.95\%}}\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 50.97\% & 16.94\% & 10.00\% & 55.38\% & 72.08\% & 19.18\% & 12.22\% & 44.61\% & 85.69\% & 19.14\% & 13.46\% & 33.10\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 67.22\% & 66.09\% & 63.88\% & 68.45\% & 61.11\% & 57.86\% & 65.55\% & 51.78\% & 56.11\% & 48.91\% & 69.23\% & 37.82\%\\
\multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 68.19\% & 68.15\% & 68.05\% & 68.24\% & 60.41\% & 59.74\% & 70.55\% & 51.81\% & 53.47\% & 49.82\% & 73.07\% & 37.79\%\\ \hline
\end{tabular}
\caption{Performance comparison against other methods for different data sets and varying levels of annotator agreement with regular training data.}
\label{tab:results_regular}
\end{table}
%\setlength{\tabcolsep}{2pt}
\renewcommand{\arraystretch}{1}
% \begin{table}[H]
    
% \centering

% \scriptsize
% \begin{tabular}{ll|rrrr|rrrr|rrrr}
% \hline
%                                                       &                             & \multicolumn{4}{c|}{\textbf{Agree 1}}                                                                                                                   & \multicolumn{4}{c|}{\textbf{Agree 2}}                                                                                                                   & \multicolumn{4}{c}{\textbf{Agree 3}}                                                                                                                   \\ \hline
% \multicolumn{1}{c|}{Set}                              & \multicolumn{1}{c|}{Method} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c}{\textit{Precision}} \\ \hline
% \multicolumn{1}{l|}{\textbf{Test 1}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.23\%}} & 48.95\% & 38.13\% & \textit{\textbf{68.33\%}} & 65.34\% & 44.94\% & 38.40\% & \textit{\textbf{54.16\%}} & 70.46\% & 42.27\% & 42.85\% & \textit{\textbf{41.70\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.53\% & 3.55\% & 1.86\% & 40.00\% & 69.53\% & 3.03\% & 1.60\% & 30.00\% & 86.27\% & 0.00\% & 0.00\% & 19.80\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 54.88\% & 67.44\% & 93.48\% & 52.75\% & 37.20\% & 58.00\% & 94.40\% & 41.86\% & 21.39\% & 47.43\% & 93.87\% & 31.73\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 52.09\% & 65.55\% & 91.16\% & 51.17\% & 35.34\% & 56.33\% & 92.00\% & 40.60\% & 20.46\% & 46.11\% & 91.83\% & 30.79\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Test 2}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.00\%}} & 50.00\% & 39.39\% & \textit{\textbf{68.42\%}} & 67.69\% & 48.90\% & 44.28\% & \textit{\textbf{54.60\%}} & 66.15\% & 35.28\% & 31.25\% & \textit{\textbf{40.51\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.23\% & 1.49\% & 0.75\% & 50.00\% & 72.30\% & 0.00\% & 0.00\% & 25.00\% & 86.92\% & 0.00\% & 0.00\% & 16.50\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 52.30\% & 63.95\% & 83.33\% & 51.88\% & 33.84\% & 52.05\% & 78.57\% & 38.91\% & 24.61\% & 42.37\% & 75.00\% & 29.53\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 53.84\% & 65.31\% & 85.60\% & 52.80\% & 35.38\% & 53.91\% & 82.85\% & 39.95\% & 26.92\% & 45.58\% & 87.50\% & 30.81\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Test 3}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.75\%}} & 53.41\% & 45.45\% & 64.74\% & 61.50\% & 47.10\% & 43.69\% & \textit{\textbf{51.07\%}} & 60.50\% & 37.25\% & 35.38\% & 39.33\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 51.25\% & 5.79\% & 3.03\% & 66.66\% & 69.50\% & 4.80\% & 2.52\% & 33.33\% & 83.00\% & 8.36\% & 4.61\% & 44.33\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 54.50\% & 66.17\% & 89.89\% & 52.35\% & 36.75\% & 55.93\% & 86.55\% & 41.32\% & 27.75\% & 48.25\% & 89.23\% & 33.07\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 54.25\% & 66.04\% & 89.89\% & 52.19\% & 37.00\% & 56.13\% & 87.39\% & 41.34\% & 27.50\% & 48.25\% & 89.23\% & 33.07\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Turk}}   & \textit{\textbf{Emo-Based}} & \textit{\textbf{65.27\%}} & 56.14\% & 44.44\% & \textit{\textbf{79.16}}\% & 67.77\% & 49.55\% & 43.88\% & \textit{\textbf{56.90\%}} & 69.44\% & 40.66\% & 40.38\% & \textit{\textbf{40.95\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 50.00\% & 4.25\% & 2.22\% & 50.00\% & 74.44\% & 6.19\% & 3.33\% & 43.75\% & 91.11\% & 6.89\% & 3.84\% & 33.12\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 53.75\% & 65.77\% & 88.88\% & 52.20\% & 34.02\% & 54.17\% & 88.33\% & 39.07\% & 20.13\% & 42.63\% & 86.53\% & 28.28\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 53.05\% & 65.29\% & 88.33\% & 51.79\% & 33.61\% & 53.77\% & 87.77\% & 38.76\% & 20.55\% & 42.97\% & 90.38\% & 28.18\%\\ \hline
% \end{tabular}
% \caption{Performance comparison against other methods for different data sets and varying levels of annotator agreement with refined training data 1.}
% \label{tab:results_refined1}
% \end{table}
% %\setlength{\tabcolsep}{2pt}
% \renewcommand{\arraystretch}{1}
% \begin{table}[H]
    
% \centering

% \scriptsize
% \begin{tabular}{ll|rrrr|rrrr|rrrr}
% \hline
%                                                       &                             & \multicolumn{4}{c|}{\textbf{Agree 1}}                                                                                                                   & \multicolumn{4}{c|}{\textbf{Agree 2}}                                                                                                                   & \multicolumn{4}{c}{\textbf{Agree 3}}                                                                                                                   \\ \hline
% \multicolumn{1}{c|}{Set}                              & \multicolumn{1}{c|}{Method} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c|}{\textit{Precision}} & \multicolumn{1}{c}{\textit{Accuracy}} & \multicolumn{1}{c}{\textit{F1}} & \multicolumn{1}{c}{\textit{Recall}} & \multicolumn{1}{c}{\textit{Precision}} \\ \hline
% \multicolumn{1}{l|}{\textbf{Test 1}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.23\%}} & 48.95\% & 38.13\% & \textit{\textbf{68.33\%}} & 65.34\% & 44.94\% & 38.40\% & \textit{\textbf{54.16\%}} & 70.46\% & 42.27\% & 42.85\% & \textit{\textbf{41.70\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.30\% & 3.53\% & 1.86\% & 36.36\% & 69.30\% & 3.02\% & 1.60\% & 27.27\% & 86.04\% & 0.00\% & 0.00\% & 18.00\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 51.39\% & 65.79\% & 93.48\% & 50.75\% & 31.86\% & 55.38\% & 91.20\% & 39.77\% & 16.51\% & 44.64\% & 87.75\% & 29.94\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 52.09\% & 65.43\% & 90.69\% & 51.18\% & 33.48\% & 55.02\% & 88.00\% & 40.02\% & 19.53\% & 44.62\% & 85.71\% & 30.16\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Test 2}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.00\%}} & 50.00\% & 39.39\% & \textit{\textbf{68.42\%}} & 67.69\% & 48.90\% & 44.28\% & \textit{\textbf{54.60\%}} & 66.15\% & 35.28\% & 31.25\% & \textit{\textbf{40.51\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.61\% & 2.96\% & 1.51\% & 66.66\% & 72.69\% & 2.77\% & 1.42\% & 50.00\% & 86.53\% & 0.00\% & 0.00\% &33.00\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 53.84\% & 65.11\% & 84.84\% & 52.83\% & 36.92\% & 54.55\% & 84.28\% & 40.33\% & 28.46\% & 46.49\% & 90.62\% & 31.26\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 54.23\% & 64.68\% & 82.57\% & 53.17\% & 38.07\% & 54.08\% & 81.42\% & 40.48\% & 31.15\% & 46.78\% & 90.62\% & 31.53\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Test 3}} & \textit{\textbf{Emo-Based}} & \textit{\textbf{60.75\%}} & 53.41\% & 45.45\% & 64.74\% & 61.50\% & 47.10\% & 43.69\% & \textit{\textbf{51.07\%}} & 60.50\% & 37.25\% & 35.38\% & 39.33\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 51.25\% & 5.79\% & 3.03\% & 66.66\% & 69.50\% & 4.80\% & 2.52\% & 50.00\% & 83.00\% & 8.36\% & 4.61\% & 44.33\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 50.75\% & 63.58\% & 86.86\% & 50.14\% & 35.00\% & 54.18\% & 84.87\% & 39.79\% & 27.00\% & 47.12\% & 89.23\% & 32.01\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 51.25\% & 63.13\% & 84.34\% & 50.45\% & 36.50\% & 53.87\% & 82.35\% & 40.03\% & 30.00\% & 47.51\% & 89.23\% & 32.37\%\\ \hline
% \multicolumn{1}{l|}{\textbf{Turk}}   & \textit{\textbf{Emo-Based}} & \textit{\textbf{65.27\%}} & 56.14\% & 44.44\% & \textit{\textbf{79.16\%}} & 67.77\% & 49.55\% & 43.88\% & \textit{\textbf{56.90\%}} & 69.44\% & 40.66\% & 40.38\% & \textit{\textbf{40.95\%}}\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{PBLGA}}     & 49.72\% & 4.23\% & 2.22\% & 44.44\% & 74.16\% & 6.14\% & 3.33\% & 38.88\% & 90.83\% & 6.80\% & 3.84\% & 29.44\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{TFIDF}}     & 52.91\% & 65.72\% & 90.27\% & 51.66\% & 33.75\% & 54.84\% & 92.22\% & 39.03\% & 19.02\% & 43.65\% & 94.23\% & 28.40\%\\
% \multicolumn{1}{l|}{}                                 & \textit{\textbf{BOW}}       & 54.44\% & 66.04\% & 88.61\% & 52.64\% & 36.11\% & 55.26\% & 90.55\% & 39.76\% & 22.22\% & 44.34\% & 94.23\% & 28.99\%\\ \hline
% \end{tabular}
% \caption{Performance comparison against other methods for different data sets and varying levels of annotator agreement with refined training data 2.}
% \label{tab:results_refined2}
% \end{table}
        \par A complete detail of performance results across all data sets is presented in Table~\ref{tab:results_regular}. The results reflect a more consistent performance from the Emo-Based method across datasets and agreement levels. It can be observed that our performance is maintained despite the ground truth becoming more strict in all testing datasets. The proposed method also dominates precision across the board reflecting a good performance of the filtering process defined in the methodology. The consistency of our mehod's performance is measured by ANOVA which is an one-way analysis of variance used to determine whether there are any statistically significant differences between the means of two or more independent groups. The result is shown in Figure~\ref{fig:anova_english}.
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/anova_english2.pdf}
            \caption{ANOVA Consistency Analysis for English Performance.}
            \label{fig:anova_english}
        \end{figure}
        \end{center}
        As we can see, Sig value are all greater then 0.05 which means they are all perform consistently.
        \par Other methods tend to lean to one class, which in the case of TF-IDF and BOW favors their recall. It can also be observed that PBLGA method presents low score for F1 and recall in some instances. This was analyzed into more detail and it was found that PBLGA labeled few texts as sarcasm and as the agree level increases also fewer sarcasm instances remain, the lower scores in the table result of there being fewer matching between these scarce labels from the classifier and the ground truth.
        %tweets and tested with 3000 tweet set. and for the second experiment is 310 tweet set
%which is annotated by four people.
        %Precision for each number of people agreement are illustrated in the figure 4.3 separately. One people agreement has the best for all of the evaluation, and 2-people agreement is better than 3-people agreement.
        %For the Amazon turk result, the result is better then the others with the 95\% approval rate of the annotators.
        %It is vary hard for people to has the same agreement on sarcasm since they are all native speakers.
        %The reason would be sarcasm can be very depend on the event, background of the user, the perspective of user, etc .
        
 
        %For precision of 1 people agreement, 2 peolpe agreement, and 3 agreement,  are lower than tokenizer method. It would be because fear, surprise, and anger are classified too much even when they are not included in tweets. If the number of emotion classified is increased, the denominator for that emotion is also increased. If they are not correct, it decreases the precision for that emotion. On contrary, recall for fear, surprise, and anger for n-grams method is better than tokenizer method. This is because these emotions are often seen in the system result. It increases the recall for these emotions.
        %Before seeing precision and recall, we count the number of emotion tags classified by
        %After learning the emotion combination, we 
        %the original system. It is illustrated in the figure 4.2. Fear, surprise, and anger are easier to be classified by n-grams method. On the other hand, the other emotions are easier to be classified by tokenizer method. The reason would be that emotion patterns for joy, trust, sadness, disgust, and anticipation are broken when n-grams is used.
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.5]{images/English_result.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection for English Comments.}
        % \end{figure}
        % \end{center}
        \section{Cross language – Chinese Sarcasm Detection Result}
        \par It is positive to see the performance behaving in a similar way for both sets of languages, which indicates that the method's components are indeed suitable for multi-lingual implementations. To perform the cross language approach, we do the sarcasm detection on Chinese language.
        \subsection{Machine Learning Result of Chinese Sarcasm}
        \par Calculation of correctly predicted times during the training process is illustrated in the Figure~\ref{fig:sar_learning_chinese}. Here is the correctly predicted times are calculated from 80\% correct to 60\% correct. The reason that the correct rate is lower than English is because Chinese significant emotion comment is very obvious which can impact on our machine learning result. The emotion combination of emotion Angry \& Haha is predicted more correctly. Hence, we choose these two combination for English sarcasm labeling.
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/Chi_cnn2.pdf}
            %\hline
            \caption{Sarcasm Learning Result of Chinese Comments.}
            \label{fig:sar_learning_chinese}
        \end{figure}
        \end{center}
        \subsection{Chinese Sarcasm Detection Result}
        To test the multilingual capabilities of our method, a Chinese classifier was implemented as defined in our methodology. The classifier performance was evaluated across the three testing sets mentioned in Subsection.
        Before seeing precision of the Chinese sarcasm detection, we calculate the improvement between the performance after the filtered approach and the original system. 
        It is illustrated in Figure~\ref{fig:filter_agree1}, Figure~\ref{fig:filter_agree2}, and Figure~\ref{fig:filter_agree3}. From one people agreement to 3-people agreement, the performance has significant improvement after the filtering approach is conducted.
        %\par FigureX presents the results for classification of sarcasm in Chinese comments on all standard metrics. Similar as in English, the classifier achieves better scores for accuracy and precision. This behavior is sustained across different levels of agreement and different testing sets which again is an indicator of a well balanced classifier.
        %The importance of these results is that the presented method learns directly from the data, no external human knowledge is added.
        
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/Chi_filter_compare21.pdf}
            %\hline
            \caption{Sarcasm Filtering Result of Chinese Comments in Test 1.}
            \label{fig:filter_agree1}
        \end{figure}
        \end{center}
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/Chi_filter_compare22.pdf}
            %\hline
            \caption{Sarcasm Filtering Result of Chinese Comments in Test 2.}
            \label{fig:filter_agree2}
        \end{figure}
        \end{center}
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/Chi_filter_compare23.pdf}
            %\hline
            \caption{Sarcasm Filtering Result of Chinese Comments in Test 3.}
            \label{fig:filter_agree3}
        \end{figure}
        \end{center}
        \subsection{Chinese Sarcasm Detection Performance}
        \par In this experiment, since in the real word the data may not always have the balanced composition percentage or situation, we gave two different dataset with different percentage of sarcasm contents to evaluate.
        Figure~\ref{fig:ch_performance} presents the results for classification of sarcasm in Chinese comments on all standard metrics. %Similar as in English, the classifier achieves better scores for accuracy and precision. This behavior is sustained across different levels of agreement and different testing sets which again is an indicator of a well balanced classifier.
        % \par Precision for each number of people agreement are illustrated in the figure 4.3 separately. As the same trend of English Data set, one people agreement has the best for all of the evaluation, and 2-people agreement is better than 3-people agreement. It is also hard for people to has the same agreement on sarcasm due to the depend on the event, background of the user, the perspective of user, etc .
        % In Chinese result, the performance is much better than English since the precision from 1 people to three people can reach at least 65\%. The reason can be that English language is used in lots of area and ethnic which might have different user background, since Chinese language is only used in East Asia especially in China and Taiwan(R.O.C) which made sarcasm content more easily to indicate.
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.45]{images/chinese_performance3.pdf}
            %\hline
            \caption{Performance of sarcasm classifier for Chinese data illustrated by Accuracy, Recall, F1-Score and Precision metrics.}
            \label{fig:ch_performance}
        \end{figure}
        \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.5]{images/Chinese_result.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Result of Chinese Comments.}
        % \end{figure}
        % \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Chi_result_test1.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Result of Chinese Comments.}
        % \end{figure}
        % \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Chi_result_test2.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Result of Chinese Comments.}
        % \end{figure}
        % \end{center}
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.4]{images/Chi_result_test3.pdf}
        %     %\hline
        %     \caption{Sarcasm Detection Result of Chinese Comments.}
        % \end{figure}
        % \end{center}
        % same area coutural so easy to indicate
        \par Similar as in English, the classifier achieves better scores for accuracy and precision. Despite the composition of the sarcasm and normal data is different,the results reflect a more consistent performance from our Emo-Based method across datasets and agreement levels. It can be observed that this performance is maintained despite the ground truth becoming more strict.
        This behavior is sustained across different levels of agreement and different testing sets which again is an indicator of a well balanced classifier.
        % The  consistency of our mehod's performance is measured by ANOVA analysis mentioned before. The result is shown in Figure~\ref{fig:anova_chinese}.
        % \begin{center}
        % \begin{figure}[H]
        %     \includegraphics[scale=0.45]{images/anova_chinese.pdf}
        %     \caption{ANOVA Consistency Analysis for Chinese Performance.}
        %     \label{fig:anova_chinese}
        % \end{figure}
        % \end{center}
        % \par As we can see, Sig value are all greater then 0.05 which means they are all perform consistently.
        \par For the  lack of performance in regards of recall, it is a reflection of the lack of a more conclusive ground truth. It is hard for people to has the same agreement on sarcasm due to the depend on the event, background of the user, the perspective of user, etc . This cause the difficulties posed during annotation. Additionally contextual knowledge by the human annotator may have a impact on understanding of a comment , and determine if it is sarcastic or not.  
        This perhaps limiting its ability to exhaustively identify the sarcastic texts and have a higher recall.
    \subsection{Sarcasm Detection Performance Evaluation with Context}
        \par To verify the system's performance and its ability of non context dependent, an additional evaluation was carried out considering a different situation with context given.
        We take Chinese testing data 3, which had been previously annotated for sarcasm as the testing data in this experiment.
        \par One of the key factors in performing this extended evaluation is that we provide the context of the fanspage where the testing data come from for the annotators to understand the background.
        If it were to perform in a better way when annotators annotate the data with the context knowledge. (e.g., fanspage detail), it would provide positive conclusions about its ability of non context dependent. The results are presented in Figure~\ref{fig:context_compar1}, Figure~\ref{fig:context_compar2}, and Figure~\ref{fig:context_compar3}.
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.4]{images/context_compare21.pdf}
            %\hline
            \caption{Performance Evaluation with Context on Agree 1 illustrated by Accuracy, Recall, F1-Score and Precision metrics.}
            \label{fig:context_compar1}
        \end{figure}
        \end{center}
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.4]{images/context_compare22.pdf}
            %\hline
            \caption{Performance Evaluation with Context on Agree 2 illustrated by Accuracy, Recall, F1-Score and Precision metrics.}
            \label{fig:context_compar2}
        \end{figure}
        \end{center}
        \begin{center}
        \begin{figure}[H]
            \includegraphics[scale=0.4]{images/context_compare23.pdf}
            %\hline
            \caption{Performance Evaluation with Context on Agree 3 illustrated by Accuracy, Recall, F1-Score and Precision metrics.}
            \label{fig:context_compar3}
        \end{figure}
        \end{center}
        \par It can be observe that the general trend is very similar to the performance obtained with the annotators without context given, which is in itself a positive indicator.
        Additionally, it actually scores higher in terms of precision, which is also important, considering that it was trained without the context corresponding to the training dataset.
        For the slightly decrease of performance in regards of recall, F1, and Accuracy, it is a reflection of the lack of a more conclusive ground truth since we provided the context of the data source. The context itself also can be understood by different people in various ways which cause less conclusive ground truth.It is hard for people to has the same agreement on sarcasm since they already have the context knowledge due to the depend on the event, background of the user, the perspective of user, etc .
        Although the ground truth can not become conclusive, but scores for F1-Measure, Recall, and Accuracy are still close to as they were in the original experiment.
\chapter{CONCLUSIONS AND FUTURE WORK}
    \par There is still much to be done in terms of developing precise, efficient, and effective methods for sarcasm detection. This work tries to bring focus to the importance of understanding the platforms being used, how users interact in them, and, more importantly, how we can make use of these behaviors when working towards identifying sarcasm. In this particular case, background knowledge of Facebook pages from news media gave some particular insights that later played an important role in the development of the method. Some examples include common behaviors of internet trolls, the usage of “Reactions” buttons, and other commenting tendencies. To the best of our knowledge, this is the first work using Facebook reactions as emotion signals for any emotion-related task.
    
    \par This work also provides a brief view on some behaviors related to the data and cultural and language differences. The extracted emotion patterns can serve as a summary of the data being dealt with and can also intuitively reflect many characteristics of the audience based on the language or expressions they use. It is also interesting to notice that the difference of emotion reactions were generally similar. Different languages reflect different uses of emotions in posts, as it was observed with the higher percentage of sad posts by English users than those in Chinese. Although it is still risky to call it a cultural difference, it can be assumed that there was a situational difference where at the time one of the language groups was more leaned towards those emotions given the ongoing events.
    
    \par Another valuable insight was provided by the difficulty in obtaining a solid ground truth, which is just a clear example on how sarcasm detection is such a hard task, as it highly depends on the intention of the sender and the understanding of the receiver. Human perception is something very hard to quantify and to teach to a computer. Nevertheless, its also interesting to see potential differences in the use and interpretation of sarcasm as denoted by the different levels of agreement during annotation.
    
    \par Extensions of this work will attempt to improve the performance metrics where it stayed behind. In order to obtain a higher recall, for instance, it is necessary to determine a more robust ground truth. Perhaps the inclusion of “experts” during annotation may aid in this task, since it has been made clear that language is not the only requirement for understanding sarcasm.Further study on the impact of context in different group of people or platform in various of cultures is also important for having a more robust ground truth.
    Additionally, other data sources must be considered, since some of the behaviors are very particular to news sites; this might pose a difficulty when trying to perform an extensive study. Other pairs of reversions of emotion can also be considered, since not all sarcasm cases are about causing laughs or anger. Data is already being collected in Spanish to develop a similar system and test if the method is indeed multi-lingual, or to evaluate to what degree it is.
    
    \par Additional studies that can be derived from this work include the study of cultural differences in sarcastic posting behavior. Likewise, evaluating which kind of posts are more prone to receive sarcastic replies can also be carried out. Finally, it is a possibility to study the role of language in the usage, understanding, and proliferation of sarcasm.


%\end{CJK}

\bibliographystyle{plain}
\bibliography{references}
\newpage
\end{CJK}
\end{document}